{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3de60f3-9557-4710-b3a0-859889739847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llmops-workshop'...\n",
      "remote: Enumerating objects: 221, done.\u001b[K\n",
      "remote: Counting objects: 100% (221/221), done.\u001b[K\n",
      "remote: Compressing objects: 100% (181/181), done.\u001b[K\n",
      "remote: Total 221 (delta 70), reused 175 (delta 33), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (221/221), 4.61 MiB | 42.90 MiB/s, done.\n",
      "Resolving deltas: 100% (70/70), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/aws-samples/llmops-workshop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d486a61-0bc5-4fa8-b778-665a3475e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker boto3 datasets pygments -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a44558a-947b-46fc-9714-ef7667c0b1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12ccf26e-bee4-4088-9731-a9e097ff87e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sagemaker\n",
    "from sagemaker.local import LocalSession\n",
    "from sagemaker.experiments.run import Run\n",
    "from sagemaker.huggingface import HuggingFace, HuggingFaceProcessor, HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch.processing import PyTorchProcessor\n",
    "\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "from datasets import load_dataset\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0166394f-7bfc-4480-9833-7c7afdeb499b",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f4ca7b-d900-42ad-ab8d-35fbebe3f8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::133114324038:role/service-role/AmazonSageMaker-ExecutionRole-20240406T005817\n",
      "sagemaker bucket: sagemaker-us-east-1-133114324038\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "645246e9-81f8-4c06-a508-f5e5805849f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"base_model_pkg_group_name\" not in locals():\n",
    "    base_model_pkg_group_name = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8bcf898-116a-4343-a183-eb215c46aef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_dataset_s3_loc: s3://sagemaker-us-east-1-133114324038/data/workshop-5449f/train\n",
      "validation_dataset_s3_loc: s3://sagemaker-us-east-1-133114324038/data/workshop-5449f/eval\n",
      "model artifact S3 location: s3://sagemaker-us-east-1-133114324038/data/workshop-5449f/model\n",
      "model evaluation output S3 location: s3://sagemaker-us-east-1-133114324038/data/workshop-5449f/modeleval\n",
      "model_id: NousResearch/Llama-2-7b-chat-hf\n",
      "base model package group name: None\n",
      "Huggingfae dataset name: hotpot_qa\n"
     ]
    }
   ],
   "source": [
    "rand_id = uuid.uuid4().hex[:5] # this is the random-id assigned for each run. \n",
    "training_dataset_s3_loc = f\"s3://{sagemaker_session_bucket}/data/workshop-{rand_id}/train\"\n",
    "validation_dataset_s3_loc = f\"s3://{sagemaker_session_bucket}/data/workshop-{rand_id}/eval\"\n",
    "model_output_s3_loc = f\"s3://{sagemaker_session_bucket}/data/workshop-{rand_id}/model\"\n",
    "model_eval_s3_loc = f\"s3://{sagemaker_session_bucket}/data/workshop-{rand_id}/modeleval\"\n",
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "hf_dataset_name = \"hotpot_qa\"\n",
    "\n",
    "print(f\"training_dataset_s3_loc: {training_dataset_s3_loc}\")\n",
    "print(f\"validation_dataset_s3_loc: {validation_dataset_s3_loc}\")\n",
    "print(f\"model artifact S3 location: {model_output_s3_loc}\")\n",
    "print(f\"model evaluation output S3 location: {model_eval_s3_loc}\")\n",
    "print(f\"model_id: {model_id}\")\n",
    "print(f\"base model package group name: {base_model_pkg_group_name}\")\n",
    "print(f\"Huggingfae dataset name: {hf_dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162825a-a1fa-4fd0-a625-76bac701c80b",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2e2b85a-aae1-4de2-b5d2-8130f16511e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    \"\"\"\n",
    "    Create a field for the given sample to store formatted llama2 prompt.\n",
    "    \n",
    "    @type  sample: Dataset sample\n",
    "    @param sample: Dataset sample\n",
    "    @rtype:   Dataset\n",
    "    @return:  Dataset sample\n",
    "    \n",
    "    \"\"\"\n",
    "    sample[\"text\"] = f\"{format_hotpot(sample)}\"\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff789e1a-4010-47f9-a0f9-f13bdeccf505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_hotpot(sample):\n",
    "    \"\"\"\n",
    "    Function that takes a single data sample derived from Huggingface datasets API: (https://huggingface.co/docs/datasets/index)\n",
    "    and formats it into llama2 prompt format. For more information about llama2 prompt format, \n",
    "    please refer to https://huggingface.co/blog/llama2#how-to-prompt-llama-2 \n",
    "    \n",
    "    An example prompt is shown in the following:\n",
    "    <s>\n",
    "      [INST] <<SYS>>\n",
    "        {{system}}\n",
    "      <</SYS>>\n",
    "\n",
    "      ### Question\n",
    "      {{question}}\n",
    "\n",
    "      ### Context\n",
    "      {{context}}[/INST] {{answer}}</s>\n",
    "    \n",
    "    @type  sample: Dataset sample\n",
    "    @param sample: dataset sample\n",
    "    @rtype:   string\n",
    "    @return:  llama2 prompt format\n",
    "    \"\"\"\n",
    "    \n",
    "    prefix = \"<s>\"\n",
    "    postfix = \"</s>\"\n",
    "    system_start_tag = \"<<SYS>>\"\n",
    "    system_end_tag = \"<</SYS>>\"\n",
    "    instruction_start_tag = \"[INST]\"\n",
    "    instruction_end_tag = \"[/INST]\"\n",
    "    context = \"\\n\".join([ \"\".join(x) for x in sample['context']['sentences']])\n",
    "    system = f\"Given the following context, answer the question as accurately as possible:\"\n",
    "    question_prompt = f\"### Question\\n{sample['question']}\"\n",
    "    context_prompt = f\"### Context\\n{context}\"\n",
    "    prompt = f\"{prefix}\\n{instruction_start_tag} {system_start_tag}\\n{system}\\n{system_end_tag}\\n\\n{question_prompt}\\n\\n{context_prompt}{instruction_end_tag} {sample['answer']} {postfix}\" \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6c140a5-ba62-40d2-8d1d-693d98dffea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99dbfb8018443d198e538836cd7ede5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = '/home/sagemaker-user/data/'\n",
    "dataset = load_dataset(hf_dataset_name, \"distractor\", split=f\"train[:10%]\")\n",
    "new_dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "training_input_path = data_path + \"/train\"\n",
    "new_dataset.save_to_disk(training_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bec1513-00a8-47d8-b62d-6ac583ee18f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97615bccf7a470d904c292ed3d81dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_dataset = load_dataset(hf_dataset_name, \"distractor\", split=f\"train[:10%]\")\n",
    "new_eval_dataset = dataset.map(template_dataset, remove_columns=list(eval_dataset.features))\n",
    "eval_input_path = data_path + \"/eval\"\n",
    "new_eval_dataset.save_to_disk(eval_input_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a69c2e7-8b2e-42dd-adf5-8bb0973a421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "torch_processor = PyTorchProcessor(\n",
    "    framework_version='2.0',\n",
    "    role=get_execution_role(),\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    # instance_type='local', # uncomment for local mode\n",
    "    instance_count=1,\n",
    "    base_job_name='frameworkprocessor-PT',\n",
    "    py_version=\"py310\",\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3065658d-211f-48c5-a5bd-630cafa8d9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.processing:Uploaded /home/sagemaker-user/llmops-workshop/src/preprocess to s3://sagemaker-us-east-1-133114324038/frameworkprocessor-PT-2024-04-21-13-26-23-667/source/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-east-1-133114324038/frameworkprocessor-PT-2024-04-21-13-26-23-667/source/runproc.sh\n",
      "INFO:sagemaker:Creating processing-job with name frameworkprocessor-PT-2024-04-21-13-26-23-667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................\u001b[34mWARNING: Skipping typing as it is not installed.\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 1))\n",
      "  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (1.26.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (14.0.1)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow-hotfix (from datasets->-r requirements.txt (line 1))\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (4.64.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 1))\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2023.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.9.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.21.2 (from datasets->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets->-r requirements.txt (line 1)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (2023.11.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3.post1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.0/542.0 kB 33.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 43.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 26.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: xxhash, pyarrow-hotfix, huggingface-hub, datasets\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.19.4\n",
      "    Uninstalling huggingface-hub-0.19.4:\n",
      "      Successfully uninstalled huggingface-hub-0.19.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed datasets-2.19.0 huggingface-hub-0.22.2 pyarrow-hotfix-0.6 xxhash-3.4.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.1 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mReceived arguments Namespace(hf_dataset_name='hotpot_qa', train_data_split='1:50', eval_data_split='51:100')\u001b[0m\n",
      "\u001b[34m#015Downloading data:   0%|          | 0.00/301M [00:00<?, ?B/s]#015Downloading data:  10%|█         | 31.5M/301M [00:00<00:01, 266MB/s]#015Downloading data:  28%|██▊       | 83.9M/301M [00:00<00:00, 365MB/s]#015Downloading data:  45%|████▌     | 136M/301M [00:00<00:00, 398MB/s] #015Downloading data:  59%|█████▉    | 178M/301M [00:00<00:00, 382MB/s]#015Downloading data:  73%|███████▎  | 220M/301M [00:00<00:00, 385MB/s]#015Downloading data:  94%|█████████▍| 283M/301M [00:00<00:00, 429MB/s]#015Downloading data: 100%|██████████| 301M/301M [00:00<00:00, 398MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading data:   0%|          | 0.00/31.1M [00:00<?, ?B/s]#015Downloading data: 100%|██████████| 31.1M/31.1M [00:00<00:00, 275MB/s]#015Downloading data: 100%|██████████| 31.1M/31.1M [00:00<00:00, 260MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading data:   0%|          | 0.00/27.5M [00:00<?, ?B/s]#015Downloading data:  38%|███▊      | 10.5M/27.5M [00:00<00:00, 58.0MB/s]#015Downloading data:  76%|███████▋  | 21.0M/27.5M [00:00<00:00, 70.3MB/s]#015Downloading data: 100%|██████████| 27.5M/27.5M [00:00<00:00, 80.4MB/s]\u001b[0m\n",
      "\u001b[34m#015Generating train split:   0%|          | 0/90447 [00:00<?, ? examples/s]#015Generating train split:   4%|▍         | 4000/90447 [00:00<00:02, 32161.15 examples/s]#015Generating train split:  10%|▉         | 9000/90447 [00:00<00:02, 36403.92 examples/s]#015Generating train split:  17%|█▋        | 15000/90447 [00:00<00:01, 40140.24 examples/s]#015Generating train split:  23%|██▎       | 21000/90447 [00:00<00:01, 41753.51 examples/s]#015Generating train split:  30%|██▉       | 27000/90447 [00:00<00:01, 42724.31 examples/s]#015Generating train split:  36%|███▋      | 33000/90447 [00:00<00:01, 43642.98 examples/s]#015Generating train split:  43%|████▎     | 39000/90447 [00:00<00:01, 43662.53 examples/s]#015Generating train split:  50%|████▉     | 45000/90447 [00:01<00:01, 43858.57 examples/s]#015Generating train split:  56%|█████▋    | 51000/90447 [00:01<00:00, 44162.53 examples/s]#015Generating train split:  63%|██████▎   | 57000/90447 [00:01<00:00, 44100.39 examples/s]#015Generating train split:  70%|██████▉   | 63000/90447 [00:01<00:00, 43288.18 examples/s]#015Generating train split:  76%|███████▋  | 69000/90447 [00:01<00:00, 44421.29 examples/s]#015Generating train split:  82%|████████▏ | 74000/90447 [00:01<00:00, 42780.83 examples/s]#015Generating train split:  88%|████████▊ | 80000/90447 [00:01<00:00, 43551.95 examples/s]#015Generating train split:  95%|█████████▌| 86000/90447 [00:01<00:00, 44293.28 examples/s]#015Generating train split: 100%|██████████| 90447/90447 [00:02<00:00, 43776.63 examples/s]#015Generating train split: 100%|██████████| 90447/90447 [00:02<00:00, 43036.36 examples/s]\u001b[0m\n",
      "\u001b[34m#015Generating validation split:   0%|          | 0/7405 [00:00<?, ? examples/s]#015Generating validation split:  68%|██████▊   | 5000/7405 [00:00<00:00, 40781.74 examples/s]#015Generating validation split: 100%|██████████| 7405/7405 [00:00<00:00, 41508.12 examples/s]\u001b[0m\n",
      "\u001b[34m#015Map:   0%|          | 0/49 [00:00<?, ? examples/s]#015Map: 100%|██████████| 49/49 [00:00<00:00, 3496.15 examples/s]\u001b[0m\n",
      "\u001b[34m#015Saving the dataset (0/1 shards):   0%|          | 0/49 [00:00<?, ? examples/s]#015Saving the dataset (1/1 shards): 100%|██████████| 49/49 [00:00<00:00, 21031.61 examples/s]#015Saving the dataset (1/1 shards): 100%|██████████| 49/49 [00:00<00:00, 19830.27 examples/s]\u001b[0m\n",
      "\u001b[34mtraining dataset uploaded to: /opt/ml/processing/train\u001b[0m\n",
      "\u001b[34m#015Saving the dataset (0/1 shards):   0%|          | 0/49 [00:00<?, ? examples/s]#015Saving the dataset (1/1 shards): 100%|██████████| 49/49 [00:00<00:00, 20019.57 examples/s]#015Saving the dataset (1/1 shards): 100%|██████████| 49/49 [00:00<00:00, 18860.32 examples/s]\u001b[0m\n",
      "\u001b[34meval dataset uploaded to: /opt/ml/processing/eval\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch_processor.run(\n",
    "    code=\"preprocess.py\",\n",
    "    source_dir=\"/home/sagemaker-user/llmops-workshop/src/preprocess\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\",\n",
    "                         source=\"/opt/ml/processing/train\",\n",
    "                         destination=training_dataset_s3_loc),\n",
    "        ProcessingOutput(output_name=\"eval_data\",\n",
    "                         source=\"/opt/ml/processing/eval\",\n",
    "                         destination=validation_dataset_s3_loc),\n",
    "\n",
    "    ],\n",
    "    arguments=[\"--train-data-split\", \"1:50\",\n",
    "               \"--eval-data-split\", \"51:100\",\n",
    "               \"--hf-dataset-name\", hf_dataset_name]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9061dbf-3cce-47d3-8a7d-f3dd51b5423f",
   "metadata": {},
   "source": [
    "# Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db97ca7b-458e-413f-957e-fc663348bfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\u001b[37m\u001b[39;49;00m\n",
      "    AutoModelForCausalLM,\u001b[37m\u001b[39;49;00m\n",
      "    AutoTokenizer,\u001b[37m\u001b[39;49;00m\n",
      "    BitsAndBytesConfig,\u001b[37m\u001b[39;49;00m\n",
      "    HfArgumentParser,\u001b[37m\u001b[39;49;00m\n",
      "    TrainingArguments,\u001b[37m\u001b[39;49;00m\n",
      "    pipeline,\u001b[37m\u001b[39;49;00m\n",
      "    logging,\u001b[37m\u001b[39;49;00m\n",
      ")\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpeft\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LoraConfig, PeftModel\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtrl\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SFTTrainer\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_from_disk\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mhuggingface\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m HuggingFaceModel\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcollection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Collection\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtqdm\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m tqdm\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msmexperiments_callback\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SageMakerExperimentsCallback\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mbotocore\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mexceptions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ClientError\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36murllib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mparse\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m urlparse\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mbitsandbytes\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mbnb\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Output directory where the model predictions and checkpoints will be stored\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "output_dir = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/model/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "base_model_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/basemodel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "model_eval_save_dir = \u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/eval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Load the entire model on the GPU 0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "device_map = \u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_arge\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"Parse the arguments.\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mgoogle/flan-t5-xl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mModel id to use for training.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mNumber of epochs to train for.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--fp16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m,  help=\u001b[33m\"\u001b[39;49;00m\u001b[33mUse fp16 for training\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--per_device_train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mper device training batch size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--per_device_eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mper device evaluation batch size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--gradient_accumulation_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of batches to accumulate for gradients before optimization step\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--gradient_checkpointing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mapply gradient checkpointing for moemory optimization at training time\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_grad_norm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.3\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mgradient norm clipping value\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m2e-4\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--weight_decay\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.001\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mweight decay\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--optimizer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mpaged_adamw_32bit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33moptimizer to use\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lr_scheduler_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mconstant\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate scheduler type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=-\u001b[34m1\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mmaximum steps to train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_ratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.03\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate warm up ratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--group_by_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mgroupping datase by length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--save_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m25\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of training steps before saving a checkpoint\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--logging_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m25\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of steps before logging the training metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_seq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mmaximum sequence length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--packing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mpack multiple examples into input sequence\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lora_r\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mLoRA attention dimension\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lora_alpha\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m16\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mAlpha parameter for LoRA scaling\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lora_dropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.1\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mDropout probability for LoRA layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--use_4bit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mWhether to use 4bit quantization\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--bnb_4bit_compute_dtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mcompute dtype for bnb_4bit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--bnb_4bit_quant_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mnf4\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mQuantization type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--use_nested_quant\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mactivate nested quantization for 4bit base models (double quantization\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lora_bias\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mnone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mwhether to use bias in the lora adapter\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--merge_weights\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mWhether to merge LoRA weights with base model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--base_model_group_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mNone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mOptional base model group name.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--region\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mus-east-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mthe region where the training job is run.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--sm_train_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/data/training\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--sm_validation_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/data/validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_eval_s3_loc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--run_experiment\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mTrue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    args = parser.parse_known_args()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m args\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32ms3_download\u001b[39;49;00m(s3_bucket, s3_object_key, local_file_name, s3_client=boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Function that downloads an object from S3 into local filesystem using boto3 library.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    meta_data = s3_client.head_object(Bucket=s3_bucket, Key=s3_object_key)\u001b[37m\u001b[39;49;00m\n",
      "    total_length = \u001b[36mint\u001b[39;49;00m(meta_data.get(\u001b[33m'\u001b[39;49;00m\u001b[33mContentLength\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m tqdm(total=total_length,  \u001b[37m\u001b[39;49;00m\n",
      "              desc=\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33msource: s3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00ms3_bucket\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00ms3_object_key\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "              bar_format=\u001b[33m\"\u001b[39;49;00m\u001b[33m{percentage:.1f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m|\u001b[39;49;00m\u001b[33m{bar:25}\u001b[39;49;00m\u001b[33m | \u001b[39;49;00m\u001b[33m{rate_fmt}\u001b[39;49;00m\u001b[33m | \u001b[39;49;00m\u001b[33m{desc}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,  \u001b[37m\u001b[39;49;00m\n",
      "              unit=\u001b[33m'\u001b[39;49;00m\u001b[33mB\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "              unit_scale=\u001b[34mTrue\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "              unit_divisor=\u001b[34m1024\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "             ) \u001b[34mas\u001b[39;49;00m pbar:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(local_file_name, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\u001b[37m\u001b[39;49;00m\n",
      "            s3_client.download_fileobj(s3_bucket, s3_object_key, f, Callback=pbar.update)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mdownload_and_untar_s3_tar\u001b[39;49;00m(destination_path, source_s3_path):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Function that downloads a file on S3, then untar the file in local filesystem.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    src_s3_bucket = source_s3_path.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[\u001b[34m2\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    src_s3_prefix = \u001b[33m\"\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join(source_s3_path.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[\u001b[34m3\u001b[39;49;00m:])\u001b[37m\u001b[39;49;00m\n",
      "    destination_file_path = os.path.join(destination_path, os.path.basename(source_s3_path))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mDownloading file from \u001b[39;49;00m\u001b[33m{\u001b[39;49;00msrc_s3_bucket\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00msrc_s3_prefix\u001b[33m}\u001b[39;49;00m\u001b[33m to \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdestination_file_path\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    s3_download(\u001b[37m\u001b[39;49;00m\n",
      "        s3_bucket=src_s3_bucket,\u001b[37m\u001b[39;49;00m\n",
      "        s3_object_key=src_s3_prefix,\u001b[37m\u001b[39;49;00m\n",
      "        local_file_name=destination_file_path\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Create a tarfile object and extract the contents to the local disk\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    tar = tarfile.open(destination_file_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    tar.extractall(path=destination_path)\u001b[37m\u001b[39;49;00m\n",
      "    tar.close()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_data_uri_from_model_package\u001b[39;49;00m(model_group_name, region=\u001b[33m\"\u001b[39;49;00m\u001b[33mus-east-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Function that retrieves the model artifact for the given model group name.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    sagemaker_session = sagemaker.session.Session(boto3.session.Session(region_name=region))\u001b[37m\u001b[39;49;00m\n",
      "    region = sagemaker_session.boto_region_name\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    sm_client = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33msagemaker\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model_packages = sm_client.list_model_packages(\u001b[37m\u001b[39;49;00m\n",
      "        ModelPackageGroupName=model_group_name\u001b[37m\u001b[39;49;00m\n",
      "    )[\u001b[33m'\u001b[39;49;00m\u001b[33mModelPackageSummaryList\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model_package_name = \u001b[36msorted\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "        [\u001b[37m\u001b[39;49;00m\n",
      "            (package[\u001b[33m'\u001b[39;49;00m\u001b[33mModelPackageVersion\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], package[\u001b[33m'\u001b[39;49;00m\u001b[33mModelPackageArn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]) \u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mfor\u001b[39;49;00m package \u001b[35min\u001b[39;49;00m model_packages \u001b[34mif\u001b[39;49;00m package[\u001b[33m'\u001b[39;49;00m\u001b[33mModelApprovalStatus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] == \u001b[33m'\u001b[39;49;00m\u001b[33mApproved\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\u001b[37m\u001b[39;49;00m\n",
      "        reverse=\u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )[-\u001b[34m1\u001b[39;49;00m][-\u001b[34m1\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mfound model package: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_package_name\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m sm_client.describe_model_package(\u001b[37m\u001b[39;49;00m\n",
      "        ModelPackageName=model_package_name\u001b[37m\u001b[39;49;00m\n",
      "    )[\u001b[33m'\u001b[39;49;00m\u001b[33mInferenceSpecification\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mContainers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[34m0\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mModelDataUrl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mquantization_config\u001b[39;49;00m(args, compute_dtype):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    At a high level, QLoRA uses 4-bit quantization to compress a pretrained language model.\u001b[39;49;00m\n",
      "\u001b[33m    This function sets up the quantization configuration for model training with QLoRA. \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    The LoRA layers are the only parameters being updated during training. \u001b[39;49;00m\n",
      "\u001b[33m    Read more about LoRA in the original LoRA paper (https://arxiv.org/abs/2106.09685). \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# 4 bit configuration\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    bnb_config = BitsAndBytesConfig(\u001b[37m\u001b[39;49;00m\n",
      "        load_in_4bit=args.use_4bit,\u001b[37m\u001b[39;49;00m\n",
      "        bnb_4bit_quant_type=args.bnb_4bit_quant_type,\u001b[37m\u001b[39;49;00m\n",
      "        bnb_4bit_compute_dtype=compute_dtype,\u001b[37m\u001b[39;49;00m\n",
      "        bnb_4bit_use_double_quant=args.use_nested_quant,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m bnb_config\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_pretrained_model\u001b[39;49;00m(args, model_name, compute_dtype):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Loads the pretrained model into GPU memory for finetuning. \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    There are 2 modes supported: \u001b[39;49;00m\n",
      "\u001b[33m    1. Directly download a pretrained model weights\u001b[39;49;00m\n",
      "\u001b[33m    from Huggingface Hub over the public internet.\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    2. Download the pretrained model weight from a base model package\u001b[39;49;00m\n",
      "\u001b[33m    registered in SageMaker Model Registry. Downloading weights from S3 \u001b[39;49;00m\n",
      "\u001b[33m    could improve the download speed significantly. \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.base_model_group_name != \u001b[33m\"\u001b[39;49;00m\u001b[33mNone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        os.makedirs(base_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        model_data_uri = model_data_uri_from_model_package(\u001b[37m\u001b[39;49;00m\n",
      "            model_group_name=args.base_model_group_name,\u001b[37m\u001b[39;49;00m\n",
      "            region=args.region\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "        download_and_untar_s3_tar(\u001b[37m\u001b[39;49;00m\n",
      "        destination_path=base_model_path, \u001b[37m\u001b[39;49;00m\n",
      "        source_s3_path=model_data_uri\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "    bnb_config = quantization_config(args, compute_dtype)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Load base model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    model = AutoModelForCausalLM.from_pretrained(\u001b[37m\u001b[39;49;00m\n",
      "        model_name,\u001b[37m\u001b[39;49;00m\n",
      "        quantization_config=bnb_config,\u001b[37m\u001b[39;49;00m\n",
      "        device_map=device_map\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    model.config.use_cache = \u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    model.config.pretraining_tp = \u001b[34m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_tokenizer\u001b[39;49;00m(args, model_name):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Loads the tokenizer for llama2 model. Tokenizer is used in the training process to \u001b[39;49;00m\n",
      "\u001b[33m    convert the input texts into tokens. \u001b[39;49;00m\n",
      "\u001b[33m    Please refer to: https://huggingface.co/docs/transformers/v4.31.0/model_doc/llama2#transformers.LlamaTokenizer \u001b[39;49;00m\n",
      "\u001b[33m    to learn more about this specific tokenizer.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer.pad_token = tokenizer.eos_token\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer.padding_side = \u001b[33m\"\u001b[39;49;00m\u001b[33mright\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[37m# Fix weird overflow issue with fp16 training\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m tokenizer\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_lora_config\u001b[39;49;00m(args, modules):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Loads QLoRA configuration for the training job. \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Load LoRA configuration\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    peft_config = LoraConfig(\u001b[37m\u001b[39;49;00m\n",
      "        lora_alpha=args.lora_alpha,\u001b[37m\u001b[39;49;00m\n",
      "        lora_dropout=args.lora_dropout,\u001b[37m\u001b[39;49;00m\n",
      "        r=args.lora_r,\u001b[37m\u001b[39;49;00m\n",
      "        bias=args.lora_bias,\u001b[37m\u001b[39;49;00m\n",
      "        task_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mCAUSAL_LM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        target_modules = modules\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m peft_config\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32ms3_upload\u001b[39;49;00m(model_evaluation_s3_path, local_file_name, s3_client=boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Uploads the given file in local file system to a specified S3 location.\u001b[39;49;00m\n",
      "\u001b[33m    This function is used for uploading the model evaluation metrics to S3.\u001b[39;49;00m\n",
      "\u001b[33m    The metrics will be used when registering the trained model with SageMaker Model Registry.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    o = urlparse(model_evaluation_s3_path)\u001b[37m\u001b[39;49;00m\n",
      "    s3_bucket = o.netloc\u001b[37m\u001b[39;49;00m\n",
      "    s3_object_key = o.path\u001b[37m\u001b[39;49;00m\n",
      "    local_base_file_name = os.path.basename(local_file_name)\u001b[37m\u001b[39;49;00m\n",
      "    s3_object_key = os.path.join(s3_object_key, local_base_file_name)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mtry\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        response = s3_client.upload_file(local_file_name, s3_bucket, s3_object_key.lstrip(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mexcept\u001b[39;49;00m ClientError \u001b[34mas\u001b[39;49;00m e:\u001b[37m\u001b[39;49;00m\n",
      "        logging.error(e)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_evaluation\u001b[39;49;00m(args, metrics):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Captures the training and evaluation metrics from the model training exercise. \u001b[39;49;00m\n",
      "\u001b[33m    The metrics will be written to file, and copied to the specifiued S3 locaiton.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    train_loss = \u001b[36mfloat\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    eval_loss = \u001b[36mfloat\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m metric \u001b[35min\u001b[39;49;00m metrics:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mtrain_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m metric:\u001b[37m\u001b[39;49;00m\n",
      "            train_loss = metric[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34melif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33meval_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m metric:\u001b[37m\u001b[39;49;00m\n",
      "            eval_loss = metric[\u001b[33m'\u001b[39;49;00m\u001b[33meval_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    evaluation_metrics =  {\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mregression_metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : {\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : train_loss\u001b[37m\u001b[39;49;00m\n",
      "            },\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33meval_loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : {\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : eval_loss\u001b[37m\u001b[39;49;00m\n",
      "            }\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        },\u001b[37m\u001b[39;49;00m\n",
      "    }\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mevaluation metrics: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mevaluation_metrics\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#Save Evaluation Report\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    pathlib.Path(model_eval_save_dir).mkdir(parents=\u001b[34mTrue\u001b[39;49;00m, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    evaluation_path = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_eval_save_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/evaluation.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(evaluation_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\u001b[37m\u001b[39;49;00m\n",
      "        f.write(json.dumps(evaluation_metrics))\u001b[37m\u001b[39;49;00m\n",
      "    s3_upload(args.model_eval_s3_loc, evaluation_path)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# COPIED FROM https://github.com/artidoro/qlora/blob/main/qlora.py\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mfind_all_linear_names\u001b[39;49;00m(model):\u001b[37m\u001b[39;49;00m\n",
      "    lora_module_names = \u001b[36mset\u001b[39;49;00m()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m name, module \u001b[35min\u001b[39;49;00m model.named_modules():\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(module, bnb.nn.Linear4bit):\u001b[37m\u001b[39;49;00m\n",
      "            names = name.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            lora_module_names.add(names[\u001b[34m0\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(names) == \u001b[34m1\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m names[-\u001b[34m1\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mlm_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m lora_module_names:  \u001b[37m# needed for 16-bit\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        lora_module_names.remove(\u001b[33m\"\u001b[39;49;00m\u001b[33mlm_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mlist\u001b[39;49;00m(lora_module_names)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtraining_function\u001b[39;49;00m(args):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mmerging weights: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.merge_weights\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    train_dataset = load_from_disk(args.sm_train_dir)\u001b[37m\u001b[39;49;00m\n",
      "    eval_dataset = load_from_disk(args.sm_validation_dir)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Load tokenizer and model with QLoRA configuration\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    compute_dtype = \u001b[36mgetattr\u001b[39;49;00m(torch, args.bnb_4bit_compute_dtype)\u001b[37m\u001b[39;49;00m\n",
      "    model_name = args.model_id\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.base_model_group_name != \u001b[33m\"\u001b[39;49;00m\u001b[33mNone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        model_name = base_model_path\u001b[37m\u001b[39;49;00m\n",
      "    model = load_pretrained_model(args, model_name, compute_dtype)\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer = load_tokenizer(args, model_name)\u001b[37m\u001b[39;49;00m\n",
      "    lora_modules = find_all_linear_names(model)\u001b[37m\u001b[39;49;00m\n",
      "    lora_config = load_lora_config(args, lora_modules)\u001b[37m\u001b[39;49;00m\n",
      "    packing = \u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m args.packing \u001b[34melse\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    fp16 = \u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m args.fp16 \u001b[34melse\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    bf16 = \u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m compute_dtype == torch.float16 \u001b[35mand\u001b[39;49;00m args.use_4bit:\u001b[37m\u001b[39;49;00m\n",
      "        major, _ = torch.cuda.get_device_capability()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m major >= \u001b[34m8\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m * \u001b[34m80\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mYour GPU supports bfloat16: accelerate training with bf16=True\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m * \u001b[34m80\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            bf16=\u001b[34mTrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Set training parameters\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    training_arguments = TrainingArguments(\u001b[37m\u001b[39;49;00m\n",
      "        output_dir=output_dir,\u001b[37m\u001b[39;49;00m\n",
      "        num_train_epochs=args.epochs,\u001b[37m\u001b[39;49;00m\n",
      "        per_device_train_batch_size=args.per_device_train_batch_size,\u001b[37m\u001b[39;49;00m\n",
      "        gradient_accumulation_steps=args.gradient_accumulation_steps,\u001b[37m\u001b[39;49;00m\n",
      "        optim=args.optimizer,\u001b[37m\u001b[39;49;00m\n",
      "        save_steps=args.save_steps,\u001b[37m\u001b[39;49;00m\n",
      "        logging_steps=args.logging_steps,\u001b[37m\u001b[39;49;00m\n",
      "        learning_rate=args.learning_rate,\u001b[37m\u001b[39;49;00m\n",
      "        weight_decay=args.weight_decay,\u001b[37m\u001b[39;49;00m\n",
      "        fp16=fp16,\u001b[37m\u001b[39;49;00m\n",
      "        bf16=bf16,\u001b[37m\u001b[39;49;00m\n",
      "        max_grad_norm=args.max_grad_norm,\u001b[37m\u001b[39;49;00m\n",
      "        max_steps=args.max_steps,\u001b[37m\u001b[39;49;00m\n",
      "        warmup_ratio=args.warmup_ratio,\u001b[37m\u001b[39;49;00m\n",
      "        group_by_length=args.group_by_length,\u001b[37m\u001b[39;49;00m\n",
      "        lr_scheduler_type=args.lr_scheduler_type,\u001b[37m\u001b[39;49;00m\n",
      "        logging_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33msteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        report_to=\u001b[33m\"\u001b[39;49;00m\u001b[33mtensorboard\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        evaluation_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33msteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Set supervised fine-tuning parameters\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.run_experiment == \u001b[33m\"\u001b[39;49;00m\u001b[33mTrue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        trainer = SFTTrainer(\u001b[37m\u001b[39;49;00m\n",
      "            model=model,\u001b[37m\u001b[39;49;00m\n",
      "            train_dataset=train_dataset,\u001b[37m\u001b[39;49;00m\n",
      "            eval_dataset=eval_dataset,\u001b[37m\u001b[39;49;00m\n",
      "            peft_config=lora_config,\u001b[37m\u001b[39;49;00m\n",
      "            dataset_text_field=\u001b[33m\"\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            max_seq_length=args.max_seq_length,\u001b[37m\u001b[39;49;00m\n",
      "            tokenizer=tokenizer,\u001b[37m\u001b[39;49;00m\n",
      "            args=training_arguments,\u001b[37m\u001b[39;49;00m\n",
      "            packing=packing,\u001b[37m\u001b[39;49;00m\n",
      "            callbacks=[SageMakerExperimentsCallback(region=args.region)]\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        trainer = SFTTrainer(\u001b[37m\u001b[39;49;00m\n",
      "            model=model,\u001b[37m\u001b[39;49;00m\n",
      "            train_dataset=train_dataset,\u001b[37m\u001b[39;49;00m\n",
      "            eval_dataset=eval_dataset,\u001b[37m\u001b[39;49;00m\n",
      "            peft_config=lora_config,\u001b[37m\u001b[39;49;00m\n",
      "            dataset_text_field=\u001b[33m\"\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            max_seq_length=args.max_seq_length,\u001b[37m\u001b[39;49;00m\n",
      "            tokenizer=tokenizer,\u001b[37m\u001b[39;49;00m\n",
      "            args=training_arguments,\u001b[37m\u001b[39;49;00m\n",
      "            packing=packing\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "    trainer.train()\u001b[37m\u001b[39;49;00m\n",
      "    evaluation_metrics = trainer.evaluate()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining metrics: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrainer.state.log_history\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    model_evaluation(args, trainer.state.log_history)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.merge_weights:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33msaving adapter weight combined with the base model weight\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# merge adapter weights with base model and save\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# save int 4 model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        new_model = \u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/lora-adapter-weights\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        trainer.model.save_pretrained(new_model, safe_serialization=\u001b[34mFalse\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# clear memory\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mdel\u001b[39;49;00m model\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mdel\u001b[39;49;00m trainer\u001b[37m\u001b[39;49;00m\n",
      "        torch.cuda.empty_cache()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        base_model = AutoModelForCausalLM.from_pretrained(\u001b[37m\u001b[39;49;00m\n",
      "            model_name,\u001b[37m\u001b[39;49;00m\n",
      "            low_cpu_mem_usage=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            return_dict=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            torch_dtype=torch.float16,\u001b[37m\u001b[39;49;00m\n",
      "            device_map=device_map,\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "        model = PeftModel.from_pretrained(base_model, new_model)\u001b[37m\u001b[39;49;00m\n",
      "        model = model.merge_and_unload()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m\u001b[39;49;00m\n",
      "        model.save_pretrained(output_dir, safe_serialization=\u001b[34mTrue\u001b[39;49;00m, max_shard_size=\u001b[33m\"\u001b[39;49;00m\u001b[33m2GB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Reload tokenizer to save it\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer.pad_token = tokenizer.eos_token\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer.padding_side = \u001b[33m\"\u001b[39;49;00m\u001b[33mright\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer.save_pretrained(output_dir)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m\u001b[39;49;00m\n",
      "        source_dir = \u001b[33m'\u001b[39;49;00m\u001b[33m./djl-inference/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# copy djl-inference files to model directory\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m f \u001b[35min\u001b[39;49;00m os.listdir(source_dir):\u001b[37m\u001b[39;49;00m\n",
      "            source_f = os.path.join(source_dir, f)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m# Copy the files to the destination folder\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            shutil.copy(source_f, output_dir)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33msaving adapter weights only\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        trainer.model.save_pretrained(output_dir, safe_serialization=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "    args, _ = parse_arge()\u001b[37m\u001b[39;49;00m\n",
      "    training_function(args)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    main()\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize /home/sagemaker-user/llmops-workshop/src/train/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c952edea-4d0b-43ce-af36-bad2bd4b44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define Training Job Name \n",
    "time_suffix = datetime.now().strftime('%y%m%d%H%M')\n",
    "experiments_name = f\"exp-{model_id.replace('/', '-')}\"\n",
    "run_name = f\"qlora-finetune-run-{time_suffix}-{rand_id}\"\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}-{rand_id}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "709b3e83-3582-45b2-974f-7e5e1c8ccbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker experiment name: exp-NousResearch-Llama-2-7b-chat-hf\n",
      "SageMaker experiment run name: qlora-finetune-run-2404211333-5449f\n",
      "SageMaker training job name: huggingface-qlora-2024-04-21-13-33-09-5449f\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                                # pre-trained model\n",
    "  'epochs': 1,                                         # number of training epochs\n",
    "  'per_device_train_batch_size': 8,                    # Batch size per GPU for training\n",
    "  'per_device_eval_batch_size' : 8,                    # Batch size per GPU for evaluation\n",
    "  'learning_rate' : 2e-4,                              # Initial learning rate (AdamW optimizer)\n",
    "  'optimizer' : \"paged_adamw_32bit\",                   # Optimizer to use\n",
    "  'logging_steps' : 5,                                 # Log every X updates steps\n",
    "  'lora_r': 64,                                        # LoRA attention dimension.\n",
    "  'lora_alpha' : 16,                                   # The alpha parameter for Lora scaling\n",
    "  'lora_dropout' : 0.1,                                # The dropout probability for Lora layers\n",
    "  'use_4bit' : True,                                   # Activate 4-bit precision base model loading\n",
    "  'bnb_4bit_compute_dtype' : \"float16\",                # Compute dtype for 4-bit base models\n",
    "  'bnb_4bit_quant_type' : \"nf4\",                       # Quantization type (fp4 or nf4)\n",
    "  'base_model_group_name' : base_model_pkg_group_name, # Base model registered in SageMaker Model Registry\n",
    "  'region': region,                                    # AWS region where the training is run\n",
    "  'model_eval_s3_loc' : model_eval_s3_loc              # S3 location for uploading the model evaluation metrics\n",
    "}\n",
    "\n",
    "print(f\"SageMaker experiment name: {experiments_name}\")\n",
    "print(f\"SageMaker experiment run name: {run_name}\")\n",
    "print(f\"SageMaker training job name: {job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b872fe8c-6d89-425e-8176-5bbde437cf94",
   "metadata": {},
   "source": [
    "# Run the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d505e136-78fc-43bf-8f04-4c7778e5dfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.experiments.run:The run (qlora-finetune-run-2404211333-5449f) under experiment (exp-nousresearch-llama-2-7b-chat-hf) already exists. Loading it.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-2024-04-21-13-33-09-5-2024-04-21-13-43-39-299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-21 13:43:39 Starting - Starting the training job...\n",
      "2024-04-21 13:43:54 Starting - Preparing the instances for training...\n",
      "2024-04-21 13:44:31 Downloading - Downloading input data...\n",
      "2024-04-21 13:44:56 Downloading - Downloading the training image..............................\n",
      "2024-04-21 13:49:47 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-04-21 13:49:57,219 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-04-21 13:49:57,238 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-21 13:49:57,248 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-04-21 13:49:57,255 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-04-21 13:49:58,976 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.16.1)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.31.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.9/116.9 kB 7.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.21.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.4.0 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl==0.4.7 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.4.7-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.40.2 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.40.2-py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (1.34.34)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (2.172.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (0.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (0.13.3)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers==4.31.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 3)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (1.60.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (2.27.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (3.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (3.20.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (0.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (3.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.35.0,>=1.34.34 in /opt/conda/lib/python3.10/site-packages (from boto3->-r requirements.txt (line 9)) (1.34.34)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->-r requirements.txt (line 9)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3->-r requirements.txt (line 9)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (2.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (6.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (4.18.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (1.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.34->boto3->-r requirements.txt (line 9)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<2.1,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.34->boto3->-r requirements.txt (line 9)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (5.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 8)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 2)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker->-r requirements.txt (line 10)) (3.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 2)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 2)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 8)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 10)) (2023.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 10)) (0.29.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 10)) (0.8.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker->-r requirements.txt (line 10)) (1.7.6.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker->-r requirements.txt (line 10)) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker->-r requirements.txt (line 10)) (21.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (0.4.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 8)) (3.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 116.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 244.2/244.2 kB 32.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 14.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.4.7-py3-none-any.whl (77 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.4/77.4 kB 10.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.5/92.5 MB 31.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 73.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: bitsandbytes, safetensors, transformers, accelerate, peft, trl\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.21.0 bitsandbytes-0.40.2 peft-0.4.0 safetensors-0.4.3 transformers-4.31.0 trl-0.4.7\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-04-21 13:50:11,193 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-21 13:50:11,193 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-21 13:50:11,237 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-21 13:50:11,266 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-21 13:50:11,295 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-21 13:50:11,305 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"base_model_group_name\": \"None\",\n",
      "        \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "        \"bnb_4bit_quant_type\": \"nf4\",\n",
      "        \"epochs\": 1,\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 5,\n",
      "        \"lora_alpha\": 16,\n",
      "        \"lora_dropout\": 0.1,\n",
      "        \"lora_r\": 64,\n",
      "        \"model_eval_s3_loc\": \"s3://sagemaker-us-east-1-133114324038/data/workshop-5449f/modeleval\",\n",
      "        \"model_id\": \"NousResearch/Llama-2-7b-chat-hf\",\n",
      "        \"optimizer\": \"paged_adamw_32bit\",\n",
      "        \"per_device_eval_batch_size\": 8,\n",
      "        \"per_device_train_batch_size\": 8,\n",
      "        \"region\": \"us-east-1\",\n",
      "        \"use_4bit\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-qlora-2024-04-21-13-33-09-5-2024-04-21-13-43-39-299\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-133114324038/huggingface-qlora-2024-04-21-13-33-09-5-2024-04-21-13-43-39-299/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"base_model_group_name\":\"None\",\"bnb_4bit_compute_dtype\":\"float16\",\"bnb_4bit_quant_type\":\"nf4\",\"epochs\":1,\"learning_rate\":0.0002,\"logging_steps\":5,\"lora_alpha\":16,\"lora_dropout\":0.1,\"lora_r\":64,\"model_eval_s3_loc\":\"s3://sagemaker-us-east-1-133114324038/data/workshop-5449f/modeleval\",\"model_id\":\"NousResearch/Llama-2-7b-chat-hf\",\"optimizer\":\"paged_adamw_32bit\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":8,\"region\":\"us-east-1\",\"use_4bit\":true}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-133114324038/huggingface-qlora-2024-04-21-13-33-09-5-2024-04-21-13-43-39-299/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"base_model_group_name\":\"None\",\"bnb_4bit_compute_dtype\":\"float16\",\"bnb_4bit_quant_type\":\"nf4\",\"epochs\":1,\"learning_rate\":0.0002,\"logging_steps\":5,\"lora_alpha\":16,\"lora_dropout\":0.1,\"lora_r\":64,\"model_eval_s3_loc\":\"s3://sagemaker-us-east-1-133114324038/data/workshop-5449f/modeleval\",\"model_id\":\"NousResearch/Llama-2-7b-chat-hf\",\"optimizer\":\"paged_adamw_32bit\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":8,\"region\":\"us-east-1\",\"use_4bit\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-qlora-2024-04-21-13-33-09-5-2024-04-21-13-43-39-299\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-133114324038/huggingface-qlora-2024-04-21-13-33-09-5-2024-04-21-13-43-39-299/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--base_model_group_name\",\"None\",\"--bnb_4bit_compute_dtype\",\"float16\",\"--bnb_4bit_quant_type\",\"nf4\",\"--epochs\",\"1\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"5\",\"--lora_alpha\",\"16\",\"--lora_dropout\",\"0.1\",\"--lora_r\",\"64\",\"--model_eval_s3_loc\",\"s3://sagemaker-us-east-1-133114324038/data/workshop-5449f/modeleval\",\"--model_id\",\"NousResearch/Llama-2-7b-chat-hf\",\"--optimizer\",\"paged_adamw_32bit\",\"--per_device_eval_batch_size\",\"8\",\"--per_device_train_batch_size\",\"8\",\"--region\",\"us-east-1\",\"--use_4bit\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_BASE_MODEL_GROUP_NAME=None\u001b[0m\n",
      "\u001b[34mSM_HP_BNB_4BIT_COMPUTE_DTYPE=float16\u001b[0m\n",
      "\u001b[34mSM_HP_BNB_4BIT_QUANT_TYPE=nf4\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=5\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=16\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=64\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_EVAL_S3_LOC=s3://sagemaker-us-east-1-133114324038/data/workshop-5449f/modeleval\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=NousResearch/Llama-2-7b-chat-hf\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=paged_adamw_32bit\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_REGION=us-east-1\u001b[0m\n",
      "\u001b[34mSM_HP_USE_4BIT=true\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --base_model_group_name None --bnb_4bit_compute_dtype float16 --bnb_4bit_quant_type nf4 --epochs 1 --learning_rate 0.0002 --logging_steps 5 --lora_alpha 16 --lora_dropout 0.1 --lora_r 64 --model_eval_s3_loc s3://sagemaker-us-east-1-133114324038/data/workshop-5449f/modeleval --model_id NousResearch/Llama-2-7b-chat-hf --optimizer paged_adamw_32bit --per_device_eval_batch_size 8 --per_device_train_batch_size 8 --region us-east-1 --use_4bit True\u001b[0m\n",
      "\u001b[34m2024-04-21 13:50:11,337 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mmerging weights: True\u001b[0m\n",
      "\u001b[34mconfig.json:   0%|          | 0.00/583 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mconfig.json: 100%|██████████| 583/583 [00:00<00:00, 5.79MB/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mmodel.safetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 94.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   0%|          | 41.9M/9.98G [00:00<00:29, 342MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   1%|          | 83.9M/9.98G [00:00<00:30, 323MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   1%|▏         | 126M/9.98G [00:00<00:29, 337MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   2%|▏         | 168M/9.98G [00:00<00:27, 359MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   2%|▏         | 210M/9.98G [00:00<00:27, 361MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   3%|▎         | 252M/9.98G [00:00<00:25, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   3%|▎         | 294M/9.98G [00:00<00:26, 371MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   3%|▎         | 336M/9.98G [00:00<00:25, 380MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   4%|▍         | 388M/9.98G [00:01<00:24, 391MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   4%|▍         | 430M/9.98G [00:01<00:24, 385MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   5%|▍         | 482M/9.98G [00:01<00:23, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   5%|▌         | 535M/9.98G [00:01<00:22, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   6%|▌         | 587M/9.98G [00:01<00:21, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   6%|▋         | 640M/9.98G [00:01<00:23, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   7%|▋         | 682M/9.98G [00:01<00:23, 403MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   7%|▋         | 724M/9.98G [00:01<00:23, 402MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   8%|▊         | 765M/9.98G [00:01<00:24, 374MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   8%|▊         | 807M/9.98G [00:02<00:29, 314MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   9%|▊         | 849M/9.98G [00:02<00:30, 296MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   9%|▉         | 891M/9.98G [00:02<00:29, 306MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:   9%|▉         | 933M/9.98G [00:02<00:29, 305MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  10%|▉         | 986M/9.98G [00:02<00:26, 341MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  10%|█         | 1.04G/9.98G [00:02<00:24, 366MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  11%|█         | 1.09G/9.98G [00:02<00:22, 392MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  11%|█▏        | 1.13G/9.98G [00:03<00:24, 363MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  12%|█▏        | 1.17G/9.98G [00:03<00:23, 368MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  12%|█▏        | 1.22G/9.98G [00:03<00:24, 362MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  13%|█▎        | 1.26G/9.98G [00:03<00:25, 348MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  13%|█▎        | 1.30G/9.98G [00:03<00:25, 340MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  13%|█▎        | 1.34G/9.98G [00:03<00:24, 351MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  14%|█▍        | 1.39G/9.98G [00:03<00:22, 379MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  14%|█▍        | 1.44G/9.98G [00:03<00:22, 382MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  15%|█▍        | 1.49G/9.98G [00:04<00:21, 388MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  15%|█▌        | 1.53G/9.98G [00:04<00:21, 391MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  16%|█▌        | 1.57G/9.98G [00:04<00:21, 397MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  16%|█▌        | 1.61G/9.98G [00:04<00:21, 393MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  17%|█▋        | 1.66G/9.98G [00:04<00:20, 400MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  17%|█▋        | 1.70G/9.98G [00:04<00:20, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  17%|█▋        | 1.74G/9.98G [00:04<00:21, 380MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  18%|█▊        | 1.79G/9.98G [00:04<00:20, 408MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  18%|█▊        | 1.85G/9.98G [00:04<00:19, 418MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  19%|█▉        | 1.90G/9.98G [00:05<00:18, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  20%|█▉        | 1.96G/9.98G [00:05<00:17, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  20%|██        | 2.01G/9.98G [00:05<00:17, 464MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  21%|██        | 2.07G/9.98G [00:05<00:20, 393MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  21%|██        | 2.11G/9.98G [00:05<00:19, 395MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  22%|██▏       | 2.16G/9.98G [00:05<00:19, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  22%|██▏       | 2.21G/9.98G [00:05<00:18, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  23%|██▎       | 2.26G/9.98G [00:05<00:17, 437MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  23%|██▎       | 2.32G/9.98G [00:06<00:17, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  24%|██▍       | 2.37G/9.98G [00:06<00:18, 418MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  24%|██▍       | 2.42G/9.98G [00:06<00:20, 373MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  25%|██▍       | 2.46G/9.98G [00:06<00:20, 375MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  25%|██▌       | 2.52G/9.98G [00:06<00:18, 401MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  26%|██▌       | 2.56G/9.98G [00:06<00:18, 391MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  26%|██▌       | 2.61G/9.98G [00:06<00:18, 393MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  27%|██▋       | 2.66G/9.98G [00:06<00:18, 404MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  27%|██▋       | 2.72G/9.98G [00:07<00:16, 430MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  28%|██▊       | 2.77G/9.98G [00:07<00:16, 441MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  28%|██▊       | 2.82G/9.98G [00:07<00:17, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  29%|██▉       | 2.87G/9.98G [00:07<00:16, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  29%|██▉       | 2.93G/9.98G [00:07<00:16, 419MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  30%|██▉       | 2.98G/9.98G [00:07<00:16, 418MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  30%|███       | 3.03G/9.98G [00:07<00:16, 416MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  31%|███       | 3.07G/9.98G [00:07<00:17, 394MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  31%|███▏      | 3.12G/9.98G [00:08<00:16, 418MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  32%|███▏      | 3.18G/9.98G [00:08<00:21, 313MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  32%|███▏      | 3.22G/9.98G [00:08<00:24, 276MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  33%|███▎      | 3.25G/9.98G [00:08<00:25, 259MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  33%|███▎      | 3.28G/9.98G [00:08<00:27, 244MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  33%|███▎      | 3.31G/9.98G [00:08<00:28, 232MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  34%|███▎      | 3.34G/9.98G [00:09<00:29, 228MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  34%|███▍      | 3.38G/9.98G [00:09<00:30, 219MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  34%|███▍      | 3.41G/9.98G [00:09<00:30, 215MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  34%|███▍      | 3.44G/9.98G [00:09<00:30, 217MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  35%|███▍      | 3.47G/9.98G [00:09<00:31, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  35%|███▌      | 3.50G/9.98G [00:09<00:30, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  35%|███▌      | 3.53G/9.98G [00:10<00:30, 212MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  36%|███▌      | 3.57G/9.98G [00:10<00:30, 211MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  36%|███▌      | 3.60G/9.98G [00:10<00:30, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  36%|███▋      | 3.63G/9.98G [00:10<00:30, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  37%|███▋      | 3.65G/9.98G [00:10<00:30, 206MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  37%|███▋      | 3.67G/9.98G [00:10<00:32, 197MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  37%|███▋      | 3.69G/9.98G [00:10<00:32, 193MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  37%|███▋      | 3.71G/9.98G [00:10<00:32, 192MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  37%|███▋      | 3.73G/9.98G [00:11<00:34, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  38%|███▊      | 3.75G/9.98G [00:11<00:34, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  38%|███▊      | 3.77G/9.98G [00:11<00:34, 181MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  38%|███▊      | 3.80G/9.98G [00:11<00:34, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  38%|███▊      | 3.82G/9.98G [00:11<00:35, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  38%|███▊      | 3.84G/9.98G [00:11<00:34, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  39%|███▊      | 3.86G/9.98G [00:11<00:34, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  39%|███▉      | 3.88G/9.98G [00:11<00:34, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  39%|███▉      | 3.90G/9.98G [00:12<00:35, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  39%|███▉      | 3.92G/9.98G [00:12<00:34, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  40%|███▉      | 3.94G/9.98G [00:12<00:35, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  40%|███▉      | 3.96G/9.98G [00:12<00:35, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  40%|███▉      | 3.98G/9.98G [00:12<00:34, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  40%|████      | 4.01G/9.98G [00:12<00:34, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  40%|████      | 4.03G/9.98G [00:12<00:34, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  41%|████      | 4.05G/9.98G [00:12<00:36, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  41%|████      | 4.07G/9.98G [00:13<00:35, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  41%|████      | 4.09G/9.98G [00:13<00:34, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  41%|████      | 4.11G/9.98G [00:13<00:34, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  41%|████▏     | 4.13G/9.98G [00:13<00:33, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  42%|████▏     | 4.15G/9.98G [00:13<00:33, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  42%|████▏     | 4.17G/9.98G [00:13<00:35, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  42%|████▏     | 4.19G/9.98G [00:13<00:34, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  42%|████▏     | 4.22G/9.98G [00:13<00:34, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  42%|████▏     | 4.24G/9.98G [00:13<00:33, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  43%|████▎     | 4.26G/9.98G [00:14<00:33, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  43%|████▎     | 4.28G/9.98G [00:14<00:34, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  43%|████▎     | 4.30G/9.98G [00:14<00:32, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  43%|████▎     | 4.32G/9.98G [00:14<00:33, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  44%|████▎     | 4.34G/9.98G [00:14<00:34, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  44%|████▎     | 4.36G/9.98G [00:14<00:32, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  44%|████▍     | 4.38G/9.98G [00:14<00:33, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  44%|████▍     | 4.40G/9.98G [00:15<00:35, 156MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  44%|████▍     | 4.42G/9.98G [00:15<00:34, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  45%|████▍     | 4.46G/9.98G [00:15<00:31, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  45%|████▍     | 4.48G/9.98G [00:15<00:32, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  45%|████▌     | 4.50G/9.98G [00:15<00:35, 155MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  45%|████▌     | 4.53G/9.98G [00:15<00:31, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  46%|████▌     | 4.55G/9.98G [00:15<00:31, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  46%|████▌     | 4.57G/9.98G [00:16<00:32, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  46%|████▌     | 4.59G/9.98G [00:16<00:32, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  46%|████▌     | 4.61G/9.98G [00:16<00:32, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  46%|████▋     | 4.63G/9.98G [00:16<00:44, 119MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  47%|████▋     | 4.67G/9.98G [00:16<00:36, 147MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  47%|████▋     | 4.70G/9.98G [00:16<00:31, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  47%|████▋     | 4.73G/9.98G [00:16<00:28, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  48%|████▊     | 4.75G/9.98G [00:17<00:28, 183MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  48%|████▊     | 4.78G/9.98G [00:17<00:26, 197MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  48%|████▊     | 4.81G/9.98G [00:17<00:24, 207MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  49%|████▊     | 4.84G/9.98G [00:17<00:27, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  49%|████▉     | 4.87G/9.98G [00:17<00:27, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  49%|████▉     | 4.89G/9.98G [00:17<00:29, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  49%|████▉     | 4.91G/9.98G [00:17<00:30, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  49%|████▉     | 4.93G/9.98G [00:18<00:31, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  50%|████▉     | 4.95G/9.98G [00:18<00:31, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  50%|████▉     | 4.97G/9.98G [00:18<00:31, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  50%|█████     | 4.99G/9.98G [00:18<00:32, 155MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  50%|█████     | 5.01G/9.98G [00:18<00:31, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  50%|█████     | 5.03G/9.98G [00:18<00:31, 155MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  51%|█████     | 5.05G/9.98G [00:18<00:31, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  51%|█████     | 5.08G/9.98G [00:19<00:29, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  51%|█████     | 5.10G/9.98G [00:19<00:31, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  51%|█████▏    | 5.12G/9.98G [00:19<00:29, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  52%|█████▏    | 5.14G/9.98G [00:19<00:29, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  52%|█████▏    | 5.16G/9.98G [00:19<00:28, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  52%|█████▏    | 5.18G/9.98G [00:19<00:29, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  52%|█████▏    | 5.20G/9.98G [00:19<00:29, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  52%|█████▏    | 5.22G/9.98G [00:19<00:29, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  53%|█████▎    | 5.24G/9.98G [00:20<00:27, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  53%|█████▎    | 5.26G/9.98G [00:20<00:27, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  53%|█████▎    | 5.28G/9.98G [00:20<00:27, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  53%|█████▎    | 5.31G/9.98G [00:20<00:29, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  53%|█████▎    | 5.33G/9.98G [00:20<00:26, 172MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  54%|█████▎    | 5.35G/9.98G [00:20<00:28, 164MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  54%|█████▍    | 5.37G/9.98G [00:20<00:27, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  54%|█████▍    | 5.39G/9.98G [00:20<00:27, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  54%|█████▍    | 5.41G/9.98G [00:21<00:26, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  54%|█████▍    | 5.43G/9.98G [00:21<00:26, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  55%|█████▍    | 5.45G/9.98G [00:21<00:26, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  55%|█████▍    | 5.47G/9.98G [00:21<00:27, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  55%|█████▌    | 5.49G/9.98G [00:21<00:27, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  55%|█████▌    | 5.52G/9.98G [00:21<00:26, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  55%|█████▌    | 5.54G/9.98G [00:21<00:25, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  56%|█████▌    | 5.56G/9.98G [00:21<00:25, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  56%|█████▌    | 5.58G/9.98G [00:22<00:25, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  56%|█████▌    | 5.60G/9.98G [00:22<00:25, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  56%|█████▋    | 5.62G/9.98G [00:22<00:24, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  57%|█████▋    | 5.64G/9.98G [00:22<00:24, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  57%|█████▋    | 5.66G/9.98G [00:22<00:24, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  57%|█████▋    | 5.68G/9.98G [00:22<00:24, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  57%|█████▋    | 5.70G/9.98G [00:22<00:24, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  57%|█████▋    | 5.73G/9.98G [00:22<00:25, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  58%|█████▊    | 5.75G/9.98G [00:23<00:25, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  58%|█████▊    | 5.78G/9.98G [00:23<00:23, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  58%|█████▊    | 5.80G/9.98G [00:23<00:23, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  58%|█████▊    | 5.82G/9.98G [00:23<00:24, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  59%|█████▊    | 5.84G/9.98G [00:23<00:24, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  59%|█████▉    | 5.86G/9.98G [00:23<00:22, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  59%|█████▉    | 5.88G/9.98G [00:23<00:23, 175MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  59%|█████▉    | 5.90G/9.98G [00:23<00:23, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  59%|█████▉    | 5.92G/9.98G [00:24<00:23, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  60%|█████▉    | 5.95G/9.98G [00:24<00:22, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  60%|█████▉    | 5.97G/9.98G [00:24<00:23, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  60%|██████    | 5.99G/9.98G [00:24<00:23, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  60%|██████    | 6.01G/9.98G [00:24<00:23, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  60%|██████    | 6.03G/9.98G [00:24<00:22, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  61%|██████    | 6.05G/9.98G [00:24<00:22, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  61%|██████    | 6.07G/9.98G [00:24<00:22, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  61%|██████    | 6.09G/9.98G [00:25<00:22, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  61%|██████▏   | 6.11G/9.98G [00:25<00:22, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  61%|██████▏   | 6.13G/9.98G [00:25<00:22, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  62%|██████▏   | 6.16G/9.98G [00:25<00:22, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  62%|██████▏   | 6.18G/9.98G [00:25<00:21, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  62%|██████▏   | 6.20G/9.98G [00:25<00:21, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  62%|██████▏   | 6.22G/9.98G [00:25<00:21, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  63%|██████▎   | 6.24G/9.98G [00:25<00:21, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  63%|██████▎   | 6.26G/9.98G [00:25<00:21, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  63%|██████▎   | 6.28G/9.98G [00:26<00:21, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  63%|██████▎   | 6.30G/9.98G [00:26<00:21, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  63%|██████▎   | 6.32G/9.98G [00:26<00:20, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  64%|██████▎   | 6.34G/9.98G [00:26<00:20, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  64%|██████▍   | 6.36G/9.98G [00:26<00:31, 116MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  64%|██████▍   | 6.42G/9.98G [00:26<00:18, 190MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  65%|██████▍   | 6.45G/9.98G [00:27<00:18, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  65%|██████▍   | 6.48G/9.98G [00:27<00:18, 185MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  65%|██████▌   | 6.50G/9.98G [00:27<00:19, 183MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  65%|██████▌   | 6.52G/9.98G [00:27<00:19, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▌   | 6.54G/9.98G [00:27<00:19, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▌   | 6.56G/9.98G [00:27<00:19, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▌   | 6.59G/9.98G [00:27<00:19, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▌   | 6.61G/9.98G [00:27<00:19, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  66%|██████▋   | 6.63G/9.98G [00:28<00:19, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  67%|██████▋   | 6.65G/9.98G [00:28<00:18, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  67%|██████▋   | 6.67G/9.98G [00:28<00:19, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  67%|██████▋   | 6.69G/9.98G [00:28<00:19, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  67%|██████▋   | 6.71G/9.98G [00:28<00:18, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  67%|██████▋   | 6.73G/9.98G [00:28<00:18, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  68%|██████▊   | 6.75G/9.98G [00:28<00:18, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  68%|██████▊   | 6.77G/9.98G [00:28<00:18, 173MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  68%|██████▊   | 6.79G/9.98G [00:29<00:18, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  68%|██████▊   | 6.82G/9.98G [00:29<00:18, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  69%|██████▊   | 6.84G/9.98G [00:29<00:17, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  69%|██████▊   | 6.86G/9.98G [00:29<00:17, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  69%|██████▉   | 6.88G/9.98G [00:29<00:17, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  69%|██████▉   | 6.90G/9.98G [00:29<00:17, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  69%|██████▉   | 6.92G/9.98G [00:29<00:17, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|██████▉   | 6.94G/9.98G [00:29<00:17, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|██████▉   | 6.96G/9.98G [00:30<00:17, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|██████▉   | 6.98G/9.98G [00:30<00:17, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|███████   | 7.00G/9.98G [00:30<00:16, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  70%|███████   | 7.03G/9.98G [00:30<00:17, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  71%|███████   | 7.05G/9.98G [00:30<00:16, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  71%|███████   | 7.07G/9.98G [00:30<00:17, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  71%|███████   | 7.09G/9.98G [00:30<00:16, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  71%|███████▏  | 7.11G/9.98G [00:30<00:16, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  71%|███████▏  | 7.13G/9.98G [00:30<00:16, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.15G/9.98G [00:31<00:16, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.17G/9.98G [00:31<00:16, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.19G/9.98G [00:31<00:15, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  72%|███████▏  | 7.21G/9.98G [00:31<00:15, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.24G/9.98G [00:31<00:16, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.26G/9.98G [00:31<00:15, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.28G/9.98G [00:31<00:15, 177MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.30G/9.98G [00:31<00:15, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  73%|███████▎  | 7.32G/9.98G [00:32<00:16, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▎  | 7.34G/9.98G [00:32<00:15, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.36G/9.98G [00:32<00:15, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.38G/9.98G [00:32<00:14, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.40G/9.98G [00:32<00:14, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  74%|███████▍  | 7.42G/9.98G [00:32<00:14, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  75%|███████▍  | 7.44G/9.98G [00:32<00:14, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  75%|███████▍  | 7.47G/9.98G [00:32<00:14, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  75%|███████▌  | 7.49G/9.98G [00:33<00:14, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  75%|███████▌  | 7.52G/9.98G [00:33<00:14, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  76%|███████▌  | 7.54G/9.98G [00:33<00:19, 127MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  76%|███████▌  | 7.56G/9.98G [00:33<00:23, 102MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  76%|███████▌  | 7.58G/9.98G [00:34<00:25, 94.8MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  76%|███████▋  | 7.62G/9.98G [00:34<00:16, 143MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  77%|███████▋  | 7.68G/9.98G [00:34<00:11, 201MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  77%|███████▋  | 7.72G/9.98G [00:34<00:09, 230MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.75G/9.98G [00:34<00:09, 245MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.78G/9.98G [00:34<00:09, 225MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  78%|███████▊  | 7.81G/9.98G [00:34<00:10, 210MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▊  | 7.84G/9.98G [00:35<00:10, 194MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▉  | 7.87G/9.98G [00:35<00:11, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▉  | 7.90G/9.98G [00:35<00:11, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  79%|███████▉  | 7.92G/9.98G [00:35<00:11, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  80%|███████▉  | 7.94G/9.98G [00:35<00:12, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  80%|███████▉  | 7.97G/9.98G [00:35<00:10, 190MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  80%|████████  | 7.99G/9.98G [00:35<00:10, 185MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  80%|████████  | 8.01G/9.98G [00:36<00:10, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████  | 8.03G/9.98G [00:36<00:10, 181MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████  | 8.05G/9.98G [00:36<00:10, 179MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████  | 8.07G/9.98G [00:36<00:10, 178MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████  | 8.10G/9.98G [00:36<00:10, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  81%|████████▏ | 8.12G/9.98G [00:36<00:10, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.14G/9.98G [00:36<00:15, 115MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.18G/9.98G [00:37<00:10, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  82%|████████▏ | 8.21G/9.98G [00:37<00:08, 198MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.24G/9.98G [00:37<00:09, 188MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.27G/9.98G [00:37<00:09, 185MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.29G/9.98G [00:37<00:09, 180MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  83%|████████▎ | 8.32G/9.98G [00:37<00:09, 180MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▎ | 8.34G/9.98G [00:37<00:09, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▍ | 8.36G/9.98G [00:38<00:09, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▍ | 8.38G/9.98G [00:38<00:09, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▍ | 8.40G/9.98G [00:38<00:09, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  84%|████████▍ | 8.42G/9.98G [00:38<00:08, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▍ | 8.44G/9.98G [00:38<00:09, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▍ | 8.46G/9.98G [00:38<00:08, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▌ | 8.48G/9.98G [00:38<00:08, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▌ | 8.50G/9.98G [00:38<00:08, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  85%|████████▌ | 8.52G/9.98G [00:39<00:08, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▌ | 8.55G/9.98G [00:39<00:08, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▌ | 8.57G/9.98G [00:39<00:08, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▌ | 8.59G/9.98G [00:39<00:08, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  86%|████████▋ | 8.61G/9.98G [00:39<00:08, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.63G/9.98G [00:39<00:08, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.65G/9.98G [00:39<00:08, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.67G/9.98G [00:39<00:08, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.69G/9.98G [00:40<00:08, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  87%|████████▋ | 8.71G/9.98G [00:40<00:07, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.73G/9.98G [00:40<00:07, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.76G/9.98G [00:40<00:07, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.78G/9.98G [00:40<00:07, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.80G/9.98G [00:40<00:07, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  88%|████████▊ | 8.82G/9.98G [00:40<00:07, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▊ | 8.84G/9.98G [00:40<00:07, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▉ | 8.86G/9.98G [00:41<00:06, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▉ | 8.88G/9.98G [00:41<00:06, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▉ | 8.90G/9.98G [00:41<00:06, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  89%|████████▉ | 8.92G/9.98G [00:41<00:06, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|████████▉ | 8.94G/9.98G [00:41<00:06, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|████████▉ | 8.97G/9.98G [00:41<00:06, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|█████████ | 8.99G/9.98G [00:41<00:06, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|█████████ | 9.01G/9.98G [00:41<00:05, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  90%|█████████ | 9.03G/9.98G [00:42<00:05, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████ | 9.05G/9.98G [00:42<00:05, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████ | 9.07G/9.98G [00:42<00:05, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████ | 9.09G/9.98G [00:42<00:05, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  91%|█████████▏| 9.11G/9.98G [00:42<00:05, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.13G/9.98G [00:42<00:05, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.15G/9.98G [00:42<00:05, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.18G/9.98G [00:43<00:04, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.20G/9.98G [00:43<00:04, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  92%|█████████▏| 9.22G/9.98G [00:43<00:04, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.24G/9.98G [00:43<00:04, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.26G/9.98G [00:43<00:04, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.28G/9.98G [00:43<00:04, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.30G/9.98G [00:43<00:04, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  93%|█████████▎| 9.32G/9.98G [00:43<00:04, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▎| 9.34G/9.98G [00:44<00:03, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▍| 9.36G/9.98G [00:44<00:03, 157MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▍| 9.38G/9.98G [00:44<00:03, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▍| 9.41G/9.98G [00:44<00:03, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  94%|█████████▍| 9.43G/9.98G [00:44<00:03, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▍| 9.45G/9.98G [00:44<00:03, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▍| 9.47G/9.98G [00:44<00:03, 163MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▌| 9.49G/9.98G [00:44<00:03, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  95%|█████████▌| 9.51G/9.98G [00:45<00:02, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▌| 9.53G/9.98G [00:45<00:02, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▌| 9.55G/9.98G [00:45<00:02, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▌| 9.57G/9.98G [00:45<00:02, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▌| 9.59G/9.98G [00:45<00:02, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  96%|█████████▋| 9.62G/9.98G [00:45<00:02, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  97%|█████████▋| 9.64G/9.98G [00:45<00:02, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  97%|█████████▋| 9.66G/9.98G [00:46<00:01, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  97%|█████████▋| 9.68G/9.98G [00:46<00:01, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  97%|█████████▋| 9.70G/9.98G [00:46<00:01, 152MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  97%|█████████▋| 9.72G/9.98G [00:46<00:01, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  98%|█████████▊| 9.74G/9.98G [00:46<00:01, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  98%|█████████▊| 9.76G/9.98G [00:46<00:01, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  98%|█████████▊| 9.78G/9.98G [00:46<00:01, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  98%|█████████▊| 9.80G/9.98G [00:47<00:01, 106MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▊| 9.84G/9.98G [00:47<00:01, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▉| 9.87G/9.98G [00:47<00:00, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▉| 9.89G/9.98G [00:47<00:00, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors:  99%|█████████▉| 9.91G/9.98G [00:47<00:00, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors: 100%|█████████▉| 9.93G/9.98G [00:48<00:00, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors: 100%|█████████▉| 9.95G/9.98G [00:48<00:00, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors: 100%|█████████▉| 9.97G/9.98G [00:48<00:00, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00001-of-00002.safetensors: 100%|██████████| 9.98G/9.98G [00:48<00:00, 206MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:48<00:48, 48.56s/it]\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   1%|          | 41.9M/3.50G [00:00<00:08, 410MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   2%|▏         | 83.9M/3.50G [00:00<00:08, 405MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   4%|▎         | 126M/3.50G [00:00<00:10, 317MB/s] #033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   5%|▍         | 168M/3.50G [00:00<00:14, 234MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   6%|▌         | 199M/3.50G [00:00<00:16, 205MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   7%|▋         | 231M/3.50G [00:00<00:16, 202MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   7%|▋         | 262M/3.50G [00:01<00:16, 195MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   8%|▊         | 283M/3.50G [00:01<00:16, 190MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   9%|▊         | 304M/3.50G [00:01<00:17, 185MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:   9%|▉         | 325M/3.50G [00:01<00:17, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  10%|▉         | 346M/3.50G [00:01<00:17, 180MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  10%|█         | 367M/3.50G [00:01<00:17, 180MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  11%|█         | 388M/3.50G [00:01<00:20, 155MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  12%|█▏        | 419M/3.50G [00:02<00:16, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  13%|█▎        | 440M/3.50G [00:02<00:16, 182MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  13%|█▎        | 461M/3.50G [00:02<00:17, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  14%|█▍        | 482M/3.50G [00:02<00:17, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  14%|█▍        | 503M/3.50G [00:02<00:17, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  15%|█▍        | 524M/3.50G [00:02<00:16, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  16%|█▌        | 545M/3.50G [00:02<00:17, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  16%|█▌        | 566M/3.50G [00:02<00:16, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  17%|█▋        | 587M/3.50G [00:03<00:16, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  17%|█▋        | 608M/3.50G [00:03<00:16, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  18%|█▊        | 629M/3.50G [00:03<00:16, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  19%|█▊        | 650M/3.50G [00:03<00:16, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  19%|█▉        | 671M/3.50G [00:03<00:16, 174MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  20%|█▉        | 692M/3.50G [00:03<00:16, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  20%|██        | 713M/3.50G [00:03<00:16, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  21%|██        | 734M/3.50G [00:03<00:16, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  22%|██▏       | 755M/3.50G [00:04<00:15, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  22%|██▏       | 776M/3.50G [00:04<00:16, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  23%|██▎       | 797M/3.50G [00:04<00:16, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  23%|██▎       | 818M/3.50G [00:04<00:16, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  24%|██▍       | 839M/3.50G [00:04<00:15, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  25%|██▍       | 860M/3.50G [00:04<00:15, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  25%|██▌       | 881M/3.50G [00:04<00:14, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  26%|██▌       | 902M/3.50G [00:04<00:14, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  26%|██▋       | 923M/3.50G [00:04<00:14, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  27%|██▋       | 944M/3.50G [00:05<00:15, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  28%|██▊       | 965M/3.50G [00:05<00:14, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  28%|██▊       | 986M/3.50G [00:05<00:14, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  29%|██▉       | 1.01G/3.50G [00:05<00:15, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  29%|██▉       | 1.03G/3.50G [00:05<00:15, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  30%|██▉       | 1.05G/3.50G [00:05<00:14, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  31%|███       | 1.07G/3.50G [00:05<00:14, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  31%|███       | 1.09G/3.50G [00:06<00:14, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  32%|███▏      | 1.11G/3.50G [00:06<00:14, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  32%|███▏      | 1.13G/3.50G [00:06<00:14, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  33%|███▎      | 1.15G/3.50G [00:06<00:14, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  34%|███▎      | 1.17G/3.50G [00:06<00:13, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  34%|███▍      | 1.20G/3.50G [00:06<00:13, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  35%|███▍      | 1.22G/3.50G [00:06<00:13, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  35%|███▌      | 1.24G/3.50G [00:06<00:13, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  36%|███▌      | 1.26G/3.50G [00:07<00:13, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  37%|███▋      | 1.28G/3.50G [00:07<00:13, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  37%|███▋      | 1.30G/3.50G [00:07<00:13, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  38%|███▊      | 1.32G/3.50G [00:07<00:12, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  38%|███▊      | 1.34G/3.50G [00:07<00:12, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  39%|███▉      | 1.36G/3.50G [00:07<00:13, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  40%|███▉      | 1.38G/3.50G [00:07<00:12, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  40%|████      | 1.41G/3.50G [00:07<00:12, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  41%|████      | 1.43G/3.50G [00:08<00:12, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  41%|████▏     | 1.45G/3.50G [00:08<00:12, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  42%|████▏     | 1.47G/3.50G [00:08<00:12, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  43%|████▎     | 1.49G/3.50G [00:08<00:11, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  43%|████▎     | 1.51G/3.50G [00:08<00:17, 112MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  44%|████▍     | 1.55G/3.50G [00:08<00:11, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  45%|████▌     | 1.58G/3.50G [00:08<00:10, 186MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  46%|████▌     | 1.61G/3.50G [00:09<00:10, 172MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  47%|████▋     | 1.64G/3.50G [00:09<00:10, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  47%|████▋     | 1.66G/3.50G [00:09<00:10, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  48%|████▊     | 1.68G/3.50G [00:09<00:10, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  49%|████▊     | 1.70G/3.50G [00:09<00:10, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  49%|████▉     | 1.72G/3.50G [00:09<00:10, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  50%|████▉     | 1.74G/3.50G [00:09<00:10, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  50%|█████     | 1.76G/3.50G [00:10<00:10, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  51%|█████     | 1.78G/3.50G [00:10<00:10, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  52%|█████▏    | 1.80G/3.50G [00:10<00:10, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  52%|█████▏    | 1.82G/3.50G [00:10<00:10, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  53%|█████▎    | 1.85G/3.50G [00:10<00:10, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  53%|█████▎    | 1.87G/3.50G [00:10<00:10, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  54%|█████▍    | 1.89G/3.50G [00:10<00:09, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  55%|█████▍    | 1.91G/3.50G [00:10<00:09, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  55%|█████▌    | 1.93G/3.50G [00:11<00:09, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  56%|█████▌    | 1.95G/3.50G [00:11<00:09, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  56%|█████▋    | 1.97G/3.50G [00:11<00:09, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  57%|█████▋    | 1.99G/3.50G [00:11<00:09, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  58%|█████▊    | 2.01G/3.50G [00:11<00:09, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  58%|█████▊    | 2.03G/3.50G [00:11<00:08, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  59%|█████▊    | 2.06G/3.50G [00:11<00:08, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  59%|█████▉    | 2.08G/3.50G [00:12<00:08, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  60%|█████▉    | 2.10G/3.50G [00:12<00:08, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  61%|██████    | 2.12G/3.50G [00:12<00:08, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  61%|██████    | 2.14G/3.50G [00:12<00:08, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  62%|██████▏   | 2.16G/3.50G [00:12<00:08, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  62%|██████▏   | 2.18G/3.50G [00:12<00:08, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  63%|██████▎   | 2.20G/3.50G [00:12<00:08, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  64%|██████▎   | 2.22G/3.50G [00:12<00:07, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  64%|██████▍   | 2.24G/3.50G [00:13<00:07, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  65%|██████▍   | 2.26G/3.50G [00:13<00:07, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  65%|██████▌   | 2.29G/3.50G [00:13<00:07, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  66%|██████▌   | 2.31G/3.50G [00:13<00:07, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  67%|██████▋   | 2.33G/3.50G [00:13<00:06, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  67%|██████▋   | 2.35G/3.50G [00:13<00:06, 169MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  68%|██████▊   | 2.37G/3.50G [00:13<00:07, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  68%|██████▊   | 2.39G/3.50G [00:13<00:06, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  69%|██████▉   | 2.41G/3.50G [00:14<00:06, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  69%|██████▉   | 2.43G/3.50G [00:14<00:06, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  70%|███████   | 2.45G/3.50G [00:14<00:06, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  71%|███████   | 2.47G/3.50G [00:14<00:06, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  71%|███████▏  | 2.50G/3.50G [00:14<00:06, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  72%|███████▏  | 2.52G/3.50G [00:14<00:06, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  72%|███████▏  | 2.54G/3.50G [00:14<00:06, 159MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  73%|███████▎  | 2.56G/3.50G [00:14<00:05, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  74%|███████▎  | 2.58G/3.50G [00:15<00:05, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  74%|███████▍  | 2.60G/3.50G [00:15<00:05, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  75%|███████▍  | 2.62G/3.50G [00:15<00:05, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  75%|███████▌  | 2.64G/3.50G [00:15<00:05, 156MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  76%|███████▌  | 2.66G/3.50G [00:15<00:05, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  77%|███████▋  | 2.68G/3.50G [00:15<00:05, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  77%|███████▋  | 2.71G/3.50G [00:15<00:04, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  78%|███████▊  | 2.73G/3.50G [00:15<00:04, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  78%|███████▊  | 2.75G/3.50G [00:16<00:04, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  79%|███████▉  | 2.77G/3.50G [00:16<00:04, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  80%|███████▉  | 2.79G/3.50G [00:16<00:04, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  80%|████████  | 2.81G/3.50G [00:16<00:04, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  81%|████████  | 2.83G/3.50G [00:16<00:04, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  81%|████████▏ | 2.85G/3.50G [00:16<00:03, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  82%|████████▏ | 2.87G/3.50G [00:16<00:03, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  83%|████████▎ | 2.89G/3.50G [00:17<00:04, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  84%|████████▍ | 2.94G/3.50G [00:17<00:03, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  84%|████████▍ | 2.96G/3.50G [00:17<00:03, 175MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  85%|████████▌ | 2.98G/3.50G [00:17<00:03, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  86%|████████▌ | 3.00G/3.50G [00:17<00:03, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  86%|████████▋ | 3.02G/3.50G [00:17<00:02, 168MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  87%|████████▋ | 3.04G/3.50G [00:17<00:02, 160MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  87%|████████▋ | 3.06G/3.50G [00:18<00:02, 168MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  88%|████████▊ | 3.08G/3.50G [00:18<00:02, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  89%|████████▊ | 3.10G/3.50G [00:18<00:02, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  89%|████████▉ | 3.12G/3.50G [00:18<00:02, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  90%|████████▉ | 3.15G/3.50G [00:18<00:02, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  90%|█████████ | 3.17G/3.50G [00:18<00:02, 158MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  91%|█████████ | 3.19G/3.50G [00:18<00:02, 118MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  93%|█████████▎| 3.24G/3.50G [00:19<00:01, 176MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  93%|█████████▎| 3.26G/3.50G [00:19<00:01, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  94%|█████████▍| 3.28G/3.50G [00:19<00:01, 170MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  94%|█████████▍| 3.30G/3.50G [00:19<00:01, 171MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  95%|█████████▍| 3.32G/3.50G [00:19<00:01, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  96%|█████████▌| 3.34G/3.50G [00:19<00:00, 162MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  96%|█████████▌| 3.37G/3.50G [00:19<00:00, 167MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  97%|█████████▋| 3.39G/3.50G [00:20<00:00, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  97%|█████████▋| 3.41G/3.50G [00:20<00:00, 165MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  98%|█████████▊| 3.43G/3.50G [00:20<00:00, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  99%|█████████▊| 3.45G/3.50G [00:20<00:00, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors:  99%|█████████▉| 3.47G/3.50G [00:20<00:00, 163MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors: 100%|█████████▉| 3.49G/3.50G [00:20<00:00, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mmodel-00002-of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:20<00:00, 168MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [01:09<00:00, 32.40s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [01:09<00:00, 34.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:29<00:29, 29.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:39<00:00, 18.27s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:39<00:00, 19.93s/it]\u001b[0m\n",
      "\u001b[34mgeneration_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mgeneration_config.json: 100%|██████████| 179/179 [00:00<00:00, 1.33MB/s]\u001b[0m\n",
      "\u001b[34mtokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer_config.json: 100%|██████████| 746/746 [00:00<00:00, 8.94MB/s]\u001b[0m\n",
      "\u001b[34mtokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 223MB/s]\u001b[0m\n",
      "\u001b[34mtokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mtokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 27.9MB/s]\u001b[0m\n",
      "\u001b[34madded_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34madded_tokens.json: 100%|██████████| 21.0/21.0 [00:00<00:00, 278kB/s]\u001b[0m\n",
      "\u001b[34mspecial_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mspecial_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 4.05MB/s]\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mYour GPU supports bfloat16: accelerate training with bf16=True\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m[sm-callback] loaded sagemaker Experiment (name: exp-nousresearch-llama-2-7b-chat-hf) with run: qlora-finetune-run-2404211333-5449f!\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/49 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 49/49 [00:00<00:00, 443.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 49/49 [00:00<00:00, 435.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/49 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 49/49 [00:00<00:00, 834.92 examples/s]\u001b[0m\n",
      "\u001b[34m[sm-callback] adding parameters to exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] TrainerState(epoch=None, global_step=0, max_steps=0, num_train_epochs=0, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': None, 'lr_scheduler': None, 'train_dataloader': None, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0, global_step=0, max_steps=7, num_train_epochs=1, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f097ca7c130>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f098407e3b0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.14285714285714285, global_step=1, max_steps=7, num_train_epochs=1, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f097ca7c130>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f098407e3b0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 1/7 [00:13<01:23, 13.89s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.14285714285714285, global_step=1, max_steps=7, num_train_epochs=1, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f097ca7c130>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f098407e3b0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.2857142857142857, global_step=2, max_steps=7, num_train_epochs=1, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f097ca7c130>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f098407e3b0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:26<01:07, 13.42s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.2857142857142857, global_step=2, max_steps=7, num_train_epochs=1, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f097ca7c130>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f098407e3b0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.42857142857142855, global_step=3, max_steps=7, num_train_epochs=1, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f097ca7c130>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f098407e3b0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:40<00:53, 13.26s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.42857142857142855, global_step=3, max_steps=7, num_train_epochs=1, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f097ca7c130>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f098407e3b0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.5714285714285714, global_step=4, max_steps=7, num_train_epochs=1, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f097ca7c130>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f098407e3b0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:53<00:39, 13.19s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.5714285714285714, global_step=4, max_steps=7, num_train_epochs=1, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f097ca7c130>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f098407e3b0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.7142857142857143, global_step=5, max_steps=7, num_train_epochs=1, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=True, should_log=True)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f097ca7c130>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f098407e3b0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [01:06<00:26, 13.15s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] logging is called exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] logging items: dict_items([('loss', 1.7948), ('learning_rate', 0.0002), ('epoch', 0.71)])\u001b[0m\n",
      "\u001b[34m{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [01:07<00:26, 13.15s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:09,  1.95s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:07<00:11,  2.77s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:11<00:09,  3.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:15<00:06,  3.44s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:19<00:03,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:20<00:00,  2.89s/it]#033[A\u001b[0m\n",
      "\u001b[34m[sm-callback] logging is called exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] logging items: dict_items([('eval_loss', 1.607375979423523), ('eval_runtime', 24.0715), ('eval_samples_per_second', 2.036), ('eval_steps_per_second', 0.291), ('epoch', 0.71)])\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.607375979423523, 'eval_runtime': 24.0715, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:21<00:00,  2.89s/it]#033[A#015 71%|███████▏  | 5/7 [01:31<00:26, 13.15s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_evaluation is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.7142857142857143, global_step=5, max_steps=7, num_train_epochs=1, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.607375979423523, 'eval_runtime': 24.0715, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.7142857142857143, global_step=5, max_steps=7, num_train_epochs=1, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.607375979423523, 'eval_runtime': 24.0715, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f097ca7c130>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f098407e3b0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f07f9b8bd90>}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.8571428571428571, global_step=6, max_steps=7, num_train_epochs=1, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.607375979423523, 'eval_runtime': 24.0715, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f097ca7c130>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f098407e3b0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f07f9b8bd90>}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [01:45<00:22, 22.16s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.8571428571428571, global_step=6, max_steps=7, num_train_epochs=1, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.607375979423523, 'eval_runtime': 24.0715, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f097ca7c130>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f098407e3b0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f07f9b8bd90>}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.0, global_step=7, max_steps=7, num_train_epochs=1, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.607375979423523, 'eval_runtime': 24.0715, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=True, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f097ca7c130>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f098407e3b0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f07f9b8bd90>}\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [01:47<00:00, 15.60s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_epoch_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.0, global_step=7, max_steps=7, num_train_epochs=1, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.607375979423523, 'eval_runtime': 24.0715, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] start: 0 ep to end: 1.0 ep!\u001b[0m\n",
      "\u001b[34mepoch_loss_values: {0.71: 1.7948}\u001b[0m\n",
      "\u001b[34mepoch_eval_loss_values: {0.71: 1.607375979423523}\u001b[0m\n",
      "\u001b[34m[sm-callback] logging is called exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] logging items: dict_items([('train_runtime', 108.8572), ('train_samples_per_second', 0.45), ('train_steps_per_second', 0.064), ('total_flos', 1062501517099008.0), ('train_loss', 1.7162607737949915), ('epoch', 1.0)])\u001b[0m\n",
      "\u001b[34m{'train_runtime': 108.8572, 'train_samples_per_second': 0.45, 'train_steps_per_second': 0.064, 'train_loss': 1.7162607737949915, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [01:49<00:00, 15.60s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [01:49<00:00, 15.68s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:09,  1.95s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:07<00:11,  2.77s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:11<00:09,  3.19s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:15<00:06,  3.44s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:19<00:03,  3.59s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:20<00:00,  2.89s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] logging is called exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] logging items: dict_items([('eval_loss', 1.5642284154891968), ('eval_runtime', 24.0693), ('eval_samples_per_second', 2.036), ('eval_steps_per_second', 0.291), ('epoch', 1.0)])\u001b[0m\n",
      "\u001b[34m[sm-callback] on_evaluation is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2404211333-5449f\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.0, global_step=7, max_steps=7, num_train_epochs=1, total_flos=1062501517099008.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.607375979423523, 'eval_runtime': 24.0715, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}, {'train_runtime': 108.8572, 'train_samples_per_second': 0.45, 'train_steps_per_second': 0.064, 'total_flos': 1062501517099008.0, 'train_loss': 1.7162607737949915, 'epoch': 1.0, 'step': 7}, {'eval_loss': 1.5642284154891968, 'eval_runtime': 24.0693, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 1.0, 'step': 7}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:22<00:00,  3.22s/it]\u001b[0m\n",
      "\u001b[34mTraining metrics: [{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.607375979423523, 'eval_runtime': 24.0715, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}, {'train_runtime': 108.8572, 'train_samples_per_second': 0.45, 'train_steps_per_second': 0.064, 'total_flos': 1062501517099008.0, 'train_loss': 1.7162607737949915, 'epoch': 1.0, 'step': 7}, {'eval_loss': 1.5642284154891968, 'eval_runtime': 24.0693, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 1.0, 'step': 7}]\u001b[0m\n",
      "\u001b[34mevaluation metrics: {'regression_metrics': {'train_loss': {'value': 1.7162607737949915}, 'eval_loss': {'value': 1.5642284154891968}}}\u001b[0m\n",
      "\u001b[34msaving adapter weight combined with the base model weight\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:29<00:29, 29.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:39<00:00, 18.16s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:39<00:00, 19.80s/it]\u001b[0m\n",
      "\u001b[34m2024-04-21 13:59:22,196 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-21 13:59:22,198 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-21 13:59:22,198 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-04-21 13:59:29 Uploading - Uploading generated training model\n",
      "2024-04-21 14:04:07 Completed - Training job completed\n",
      "Training seconds: 1175\n",
      "Billable seconds: 1175\n"
     ]
    }
   ],
   "source": [
    "with Run(\n",
    "    experiment_name=experiments_name,\n",
    "    run_name=run_name,\n",
    "    sagemaker_session=sess\n",
    ") as run:\n",
    "\n",
    "    # create the Estimator\n",
    "    huggingface_estimator = HuggingFace(\n",
    "        entry_point='train.py',         # train script\n",
    "        source_dir='/home/sagemaker-user/llmops-workshop/src/train',         # directory which includes all the files needed for training\n",
    "        instance_type='ml.g5.xlarge', # instances type used for the training job\n",
    "        # instance_type='local_gpu',      # use local \n",
    "        instance_count=1,               # the number of instances used for training\n",
    "        base_job_name=job_name,         # the name of the training job\n",
    "        role=get_execution_role(),      # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "        volume_size= 200,    # the size of the EBS volume in GB\n",
    "        transformers_version='4.28.1',    # the transformers version used in the training job\n",
    "        pytorch_version='2.0.0',          # the pytorch_version version used in the training job\n",
    "        py_version='py310',             # the python version used in the training job\n",
    "        hyperparameters= hyperparameters,\n",
    "        environment={ \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "        sagemaker_session=sess,         # specifies a sagemaker session object\n",
    "        output_path=model_output_s3_loc # s3 location for model artifact,\n",
    "    )\n",
    "    \n",
    "    # define a data input dictonary with our uploaded s3 uris\n",
    "    data = { 'training': training_dataset_s3_loc,\n",
    "             'validation': validation_dataset_s3_loc}\n",
    "\n",
    "    # starting the train job with our uploaded datasets as input\n",
    "    huggingface_estimator.fit(data, wait=True)\n",
    "    run.log_parameters(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f05df8e-580e-42d4-9f22-431131dde1f0",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8cb0ea5b-8e31-4d2e-8dce-08e9f6498168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm_image = sagemaker.image_uris.retrieve(\n",
    "    \"djl-deepspeed\", region=region, version=\"0.23.0\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0e94961-d6d5-4cc3-ae1e-a23f1c7449bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 3600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bcb89411-11f8-4964-a7c7-fbf0bbf50cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create HuggingFaceModel with the a DJI image uri\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=huggingface_estimator.model_data,\n",
    "    image_uri=llm_image,\n",
    "    transformers_version=\"4.28.1\",\n",
    "    pytorch_version=\"2.0.0\",\n",
    "    py_version=\"py310\",\n",
    "    model_server_workers=1,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e84b0b9c-d678-40e0-bab1-ef88de94b5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: djl-inference-2024-04-21-14-05-22-248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint name: llama2-7b-djl-deepspeed-ec66e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name llama2-7b-djl-deepspeed-ec66e\n",
      "INFO:sagemaker:Creating endpoint with name llama2-7b-djl-deepspeed-ec66e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name_random_id = uuid.uuid4().hex[:5]\n",
    "endpoint_name = f\"llama2-7b-djl-deepspeed-{endpoint_name_random_id}\"\n",
    "\n",
    "print(f\"endpoint name: {endpoint_name}\")\n",
    "llm = huggingface_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, \n",
    "  endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670cc98-1281-4310-a530-467a72aba194",
   "metadata": {},
   "source": [
    "# Test the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "30f319ea-0a2a-454f-8cb4-22ea6777dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>\n",
    "[INST] <<SYS>>\n",
    "{{system}}\n",
    "<</SYS>>\n",
    "\n",
    "### Question\n",
    "{{question}}\n",
    "\n",
    "### Context\n",
    "{{context}}[/INST] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed9ee5d2-cb80-4af5-9e57-1289407bb52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"Given the following context, answer the question as accurately as possible:\"\n",
    "\n",
    "def build_llama2_prompt(message):\n",
    "    question = message['question']\n",
    "    context = message['context']\n",
    "    formatted_message = prompt_template.replace(\"{{system}}\", system_message)\n",
    "    formatted_message = formatted_message.replace(\"{{question}}\", question)\n",
    "    formatted_message = formatted_message.replace(\"{{context}}\", context)\n",
    "    return formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c16ec903-d42d-4ceb-87e4-d379e98cfe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = {}\n",
    "message['question'] = \"The Oberoi family is part of a hotel company that has a head office in what city?\"\n",
    "message['context'] = \"\"\"The Ritz-Carlton Jakarta is a hotel and skyscraper in Jakarta, Indonesia and 14th Tallest building in Jakarta. It is located in city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott Hotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two towers that comprises a hotel and the Airlangga Apartment respectively. The hotel was opened in 2005.\n",
    "The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group.\n",
    "The Oberoi Group is a hotel company with its head office in Delhi. Founded in 1934, the company owns and/or operates 30+ luxury hotels and two river cruise ships in six countries, primarily under its Oberoi Hotels & Resorts and Trident Hotels brands.\n",
    "The 289th Military Police Company was activated on 1 November 1994 and attached to Hotel Company, 3rd Infantry (The Old Guard), Fort Myer, Virginia. Hotel Company is the regiment\\'s specialty company.\\nThe Glennwanis Hotel is a historic hotel in Glennville, Georgia, Tattnall County, Georgia, built on the site of the Hughes Hotel. The hotel is located at 209-215 East Barnard Street. The old Hughes Hotel was built out of Georgia pine circa 1905 and burned in 1920. The Glennwanis was built in brick in 1926. The local Kiwanis club led the effort to get the replacement hotel built, and organized a Glennville Hotel Company with directors being local business leaders. The wife of a local doctor won a naming contest with the name \"Glennwanis Hotel\", a suggestion combining \"Glennville\" and \"Kiwanis\".'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8a52733-7fd7-4b3a-86d7-521d83611033",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = {}\n",
    "message['question'] = \"Who ordered the wiretapping of Democratic Party headquarters\"\n",
    "message['context'] = \"\"\"On January 27, 1972, G. Gordon Liddy, Finance Counsel for the Committee for the Re-Election of the President (CRP) and former aide to John Ehrlichman, presented a campaign intelligence plan to CRP's acting chairman Jeb Stuart Magruder, Attorney General John Mitchell, and Presidential Counsel John Dean that involved extensive illegal activities against the Democratic Party. According to Dean, this marked \"the opening scene of the worst political scandal of the twentieth century and the beginning of the end of the Nixon presidency\".[19]: p. xvii \n",
    "Mitchell viewed the plan as unrealistic. Two months later, Mitchell approved a reduced version of the plan, including burglarizing the Democratic National Committee's (DNC) headquarters at the Watergate Complex in Washington, D.C. to photograph campaign documents and install listening devices in telephones. Liddy was nominally in charge of the operation,[citation needed] but has since insisted that he was duped by both Dean and at least two of his subordinates, which included former CIA officers E. Howard Hunt and James McCord, the latter of whom was serving as then-CRP Security Coordinator after John Mitchell had by then resigned as attorney general to become the CRP chairman.[20][21]\n",
    "In May, McCord assigned former FBI agent Alfred C. Baldwin III to carry out the wiretapping and monitor the telephone conversations afterward.[22] McCord testified that he selected Baldwin's name from a registry published by the FBI's Society of Former Special Agents to work for the Committee to Re-elect President Nixon. Baldwin first served as bodyguard to Martha Mitchell—John Mitchell's wife, who was living in Washington. Baldwin accompanied Martha Mitchell to Chicago. Eventually the committee replaced Baldwin with another security man.[citation needed]\n",
    "On May 11, McCord arranged for Baldwin, whom investigative reporter Jim Hougan described as \"somehow special and perhaps well known to McCord\", to stay at the Howard Johnson's motel across the street from the Watergate complex.[23] Room 419 was booked in the name of McCord's company.[23] At the behest of Liddy and Hunt, McCord and his team of burglars prepared for their first Watergate break-in, which began on May 28.[24]\n",
    "Two phones inside the DNC headquarters' offices were said to have been wiretapped.[25] One was Robert Spencer Oliver's phone. At the time, Oliver was working as the executive director of the Association of State Democratic Chairmen. The other phone belonged to DNC chairman Larry O'Brien. The FBI found no evidence that O'Brien's phone was bugged;[26] however, it was determined that an effective listening device was installed in Oliver's phone. While successful with installing the listening devices, the committee agents soon determined that they needed repairs. They plotted a second \"burglary\" in order to take care of the situation.[25]\n",
    "Sometime after midnight on Saturday, June 17, 1972, Watergate Complex security guard Frank Wills noticed tape covering the latches on some of the complex's doors leading from the underground parking garage to several offices, which allowed the doors to close but stay unlocked. He removed the tape, believing it was nothing.[27] When he returned a short time later and discovered that someone had retaped the locks, he called the police.[27]\n",
    "Police dispatched an unmarked police car with three plainclothes officers, Sgt. Paul W. Leeper, Officer John B. Barrett, and Officer Carl M. Shoffler, who were working the overnight shift; they were often referred to as the \"bum squad\" because they often dressed undercover as hippies and were on the lookout for drug deals and other street crimes.[28] Alfred Baldwin, on \"spotter\" duty at the Howard Johnson's hotel across the street, was distracted watching the film Attack of the Puppet People on TV and did not observe the arrival of the police car in front of the Watergate building,[28] nor did he see the plainclothes officers investigating the DNC's sixth floor suite of 29 offices. By the time Baldwin finally noticed unusual activity on the sixth floor and radioed the burglars, it was already too late.[28]\n",
    "The police apprehended five men, later identified as Virgilio Gonzalez, Bernard Barker, James McCord, Eugenio Martínez, and Frank Sturgis.[20] They were criminally charged with attempted burglary and attempted interception of telephone and other communications. The Washington Post reported the day after the burglary that, \"police found lock-picks and door jimmies, almost $2,300 in cash, most of it in $100 bills with the serial numbers in sequence ... a shortwave receiver that could pick up police calls, 40 rolls of unexposed film, two 35 millimeter cameras and three pen-sized tear gas guns\".[29] The Post would later report that the actual amount of cash was $5,300.[30]\n",
    "The following morning, Sunday, June 18, G. Gordon Liddy called Jeb Magruder in Los Angeles and informed him that \"the four men arrested with McCord were Cuban freedom fighters, whom Howard Hunt recruited\". Initially, Nixon's organization and the White House quickly went to work to cover up the crime and any evidence that might have damaged the president and his reelection.'On September 15, 1972, a grand jury indicted the five office burglars, as well as Hunt and Liddy,[32] for conspiracy, burglary, and violation of federal wiretapping laws. The burglars were tried by a jury, with Judge John Sirica officiating, and pled guilty or were convicted on January 30, 1973.[33]\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be8a2585-05ad-4d74-9fe6-54f5bcc1828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = build_llama2_prompt(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1eb636ed-9d3a-43bd-9c74-9e28ab9c9345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "[INST] <<SYS>>\n",
      "Given the following context, answer the question as accurately as possible:\n",
      "<</SYS>>\n",
      "\n",
      "### Question\n",
      "Who ordered the wiretapping of Democratic Party headquarters\n",
      "\n",
      "### Context\n",
      "On January 27, 1972, G. Gordon Liddy, Finance Counsel for the Committee for the Re-Election of the President (CRP) and former aide to John Ehrlichman, presented a campaign intelligence plan to CRP's acting chairman Jeb Stuart Magruder, Attorney General John Mitchell, and Presidential Counsel John Dean that involved extensive illegal activities against the Democratic Party. According to Dean, this marked \"the opening scene of the worst political scandal of the twentieth century and the beginning of the end of the Nixon presidency\".[19]: p. xvii \n",
      "Mitchell viewed the plan as unrealistic. Two months later, Mitchell approved a reduced version of the plan, including burglarizing the Democratic National Committee's (DNC) headquarters at the Watergate Complex in Washington, D.C. to photograph campaign documents and install listening devices in telephones. Liddy was nominally in charge of the operation,[citation needed] but has since insisted that he was duped by both Dean and at least two of his subordinates, which included former CIA officers E. Howard Hunt and James McCord, the latter of whom was serving as then-CRP Security Coordinator after John Mitchell had by then resigned as attorney general to become the CRP chairman.[20][21]\n",
      "In May, McCord assigned former FBI agent Alfred C. Baldwin III to carry out the wiretapping and monitor the telephone conversations afterward.[22] McCord testified that he selected Baldwin's name from a registry published by the FBI's Society of Former Special Agents to work for the Committee to Re-elect President Nixon. Baldwin first served as bodyguard to Martha Mitchell—John Mitchell's wife, who was living in Washington. Baldwin accompanied Martha Mitchell to Chicago. Eventually the committee replaced Baldwin with another security man.[citation needed]\n",
      "On May 11, McCord arranged for Baldwin, whom investigative reporter Jim Hougan described as \"somehow special and perhaps well known to McCord\", to stay at the Howard Johnson's motel across the street from the Watergate complex.[23] Room 419 was booked in the name of McCord's company.[23] At the behest of Liddy and Hunt, McCord and his team of burglars prepared for their first Watergate break-in, which began on May 28.[24]\n",
      "Two phones inside the DNC headquarters' offices were said to have been wiretapped.[25] One was Robert Spencer Oliver's phone. At the time, Oliver was working as the executive director of the Association of State Democratic Chairmen. The other phone belonged to DNC chairman Larry O'Brien. The FBI found no evidence that O'Brien's phone was bugged;[26] however, it was determined that an effective listening device was installed in Oliver's phone. While successful with installing the listening devices, the committee agents soon determined that they needed repairs. They plotted a second \"burglary\" in order to take care of the situation.[25]\n",
      "Sometime after midnight on Saturday, June 17, 1972, Watergate Complex security guard Frank Wills noticed tape covering the latches on some of the complex's doors leading from the underground parking garage to several offices, which allowed the doors to close but stay unlocked. He removed the tape, believing it was nothing.[27] When he returned a short time later and discovered that someone had retaped the locks, he called the police.[27]\n",
      "Police dispatched an unmarked police car with three plainclothes officers, Sgt. Paul W. Leeper, Officer John B. Barrett, and Officer Carl M. Shoffler, who were working the overnight shift; they were often referred to as the \"bum squad\" because they often dressed undercover as hippies and were on the lookout for drug deals and other street crimes.[28] Alfred Baldwin, on \"spotter\" duty at the Howard Johnson's hotel across the street, was distracted watching the film Attack of the Puppet People on TV and did not observe the arrival of the police car in front of the Watergate building,[28] nor did he see the plainclothes officers investigating the DNC's sixth floor suite of 29 offices. By the time Baldwin finally noticed unusual activity on the sixth floor and radioed the burglars, it was already too late.[28]\n",
      "The police apprehended five men, later identified as Virgilio Gonzalez, Bernard Barker, James McCord, Eugenio Martínez, and Frank Sturgis.[20] They were criminally charged with attempted burglary and attempted interception of telephone and other communications. The Washington Post reported the day after the burglary that, \"police found lock-picks and door jimmies, almost $2,300 in cash, most of it in $100 bills with the serial numbers in sequence ... a shortwave receiver that could pick up police calls, 40 rolls of unexposed film, two 35 millimeter cameras and three pen-sized tear gas guns\".[29] The Post would later report that the actual amount of cash was $5,300.[30]\n",
      "The following morning, Sunday, June 18, G. Gordon Liddy called Jeb Magruder in Los Angeles and informed him that \"the four men arrested with McCord were Cuban freedom fighters, whom Howard Hunt recruited\". Initially, Nixon's organization and the White House quickly went to work to cover up the crime and any evidence that might have damaged the president and his reelection.'On September 15, 1972, a grand jury indicted the five office burglars, as well as Hunt and Liddy,[32] for conspiracy, burglary, and violation of federal wiretapping laws. The burglars were tried by a jury, with Judge John Sirica officiating, and pled guilty or were convicted on January 30, 1973.[33][/INST] \n"
     ]
    }
   ],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "09bbe40a-d6a6-4c70-a6c5-1e11d13a9487",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "  }\n",
    "\n",
    "output = llm.predict({\"text\":input, \"properties\" : params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b84684d0-aebb-489a-acbf-d800a78fc0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the information provided in the text, it appears that the order to wiretap Democratic Party headquarters came from high-ranking officials within the Nixon administration, including Attorney General John Mitchell and Presidential Counsel John Dean. However, the exact extent of their involvement and the specific individuals who authorized the wiretapping are not clearly stated in the passage.\n",
      "\n",
      "According to the text, G. Gordon Liddy, a lawyer and former aide to John Ehrlichman, presented a campaign intelligence plan to CRP's acting chairman Jeb Stuart Magruder, Attorney General John Mitchell, and Presidential Counsel John Dean in January 1972. This plan included extensive illegal activities against the Democratic Party, including wiretapping their headquarters. Two months later, Mitchell approved a reduced version of the plan, which included burglarizing the Democratic National Committee's (DNC) headquarters at the Watergate Complex in Washington, D.C. to photograph campaign documents and install listening devices in telephones.\n",
      "\n",
      "While the passage does not explicitly state who authorized the wiretapping of Democratic Party headquarters, it suggests that high-ranking officials within the Nixon administration were involved in the planning and execution of the illegal activities. Additionally, the passage notes that the FBI found no evidence that DNC chairman Larry O'Brien's phone was bugged, which suggests that the wiretapping may have been targeted specifically at the Democratic Party.\n"
     ]
    }
   ],
   "source": [
    "print(output['outputs'][0][\"generated_text\"][len(input):]) # automatically removed the bos_token and eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be1c6caa-225c-4a38-b464-643e2b8b3f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: llama2-7b-djl-deepspeed-ec66e\n",
      "INFO:sagemaker:Deleting endpoint with name: llama2-7b-djl-deepspeed-ec66e\n"
     ]
    }
   ],
   "source": [
    "llm.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
